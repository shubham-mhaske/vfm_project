# Visual Foundation Model for Histopathology Image Segmentation

This project provides a comprehensive framework for utilizing and evaluating Visual Foundation Models (VFMs) like the Segment Anything Model (SAM/SAM2) for medical image segmentation, with a specific focus on breast cancer histology images from the BCSS dataset.

The framework supports various prompting strategies for zero-shot segmentation, including hard-coded prompts and LLM-generated text and multimodal prompts. It also includes scripts for fine-tuning SAM, evaluating segmentation performance, and running experiments in a reproducible manner, both locally and on High-Performance Computing (HPC) clusters.

## Features

- **Zero-Shot Segmentation**: Evaluate pre-trained SAM with different prompting strategies.
- **LLM-Powered Prompt Engineering**: Automatically generate text and multimodal prompts using Large Language Models (LLMs) like Gemini.
- **Fine-Tuning**: Fine-tune the SAM model on the BCSS dataset for improved performance.
- **Comprehensive Evaluation**: Calculate segmentation metrics (like Dice score) and class-based performance.
- **Reproducible Experiments**: Standardized scripts and configuration files for running experiments.
- **HPC Ready**: Includes a SLURM script for running jobs on GPU nodes.

## Directory Structure

```
/
├───configs/                # Configuration files for experiments.
│   └───prompts/            # JSON files with generated prompts for SAM.
├───data/                   # Root directory for datasets (not tracked by Git).
│   └───bcss/               # BCSS dataset images and masks.
├───results/                # Output directory for experiment results (not tracked by Git).
│   └───[experiment_name]/  # Each experiment saves its metrics and outputs here.
├───src/                    # All Python source code.
│   ├───main.py             # Main script to run all experiments.
│   ├───sam_segmentation.py # Core logic for SAM-based segmentation.
│   ├───evaluation.py       # Logic for evaluating segmentation masks.
│   ├───train_sam.py        # Script for fine-tuning SAM.
│   ├───dataset.py          # PyTorch Dataset and DataLoader definitions.
│   ├───prompt_generation.py # Scripts for generating prompts using LLMs.
│   └───...
├───sam2/                   # Git submodule for the SAM2 model.
├───CrowdsourcingDataset-Amgadetal2019/ # Git submodule for BCSS dataset download scripts.
├───models/                 # Directory for storing model checkpoints (not tracked by Git).
├───notebooks/              # Jupyter notebooks for exploration and analysis.
├───run_gpu.slurm           # SLURM script for running jobs on an HPC cluster.
└───README.md               # This file.
```

## Setup and Installation

Follow these steps to set up the project environment.

**1. Clone the Main Repository**
```bash
git clone <your-repository-url>
cd <project-directory>
```

**2. Set Up Nested Submodules**
This project uses Git submodules for `sam2` and the dataset downloader. Initialize them with:
```bash
git submodule update --init --recursive
```

**3. Create a Python Environment and Install Dependencies**
It is recommended to use a virtual environment.
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**4. Download Pre-trained Models**
- **SAM2 Models**: The `sam2` submodule includes a script to download its models.
  ```bash
  bash sam2/checkpoints/download_ckpts.sh
  ```
- **Original SAM Model**: Download the official ViT-H SAM model.
  ```bash
  mkdir -p models/sam_checkpoints
  wget -P models/sam_checkpoints https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
  ```

**5. Download the Dataset**
The dataset is downloaded using the script from the `CrowdsourcingDataset-Amgadetal2019` submodule.
```bash
# Navigate to the submodule directory
cd CrowdsourcingDataset-Amgadetal2019

# Install its specific dependencies
pip install girder_client numpy imageio scikit-image

# Run the download script (you may need to register for an account)
python download_crowdsource_dataset.py

# Return to the project root
cd ..

# Organize the data into the project's data directory
# Create the destination folders
mkdir -p data/bcss/images data/bcss/masks

# Move the downloaded files
mv CrowdsourcingDataset-Amgadetal2019/data/wsis/* data/bcss/images/
mv CrowdsourcingDataset-Amgadetal2019/data/annotations/* data/bcss/masks/
```

## Running Experiments

All experiments are managed through `src/main.py`. You can select the experiment to run using the `--experiment` flag.

**Available Experiments:**
- `zeroshot_sam_hardcoded`: Zero-shot SAM with hard-coded point prompts.
- `zeroshot_sam_llm_text`: Zero-shot SAM with text prompts generated by an LLM.
- `zeroshot_sam_llm_multimodal`: Zero-shot SAM with multimodal (text + image) prompts from an LLM.
- `finetune_sam`: Fine-tune the SAM model on the BCSS dataset.

**Example Usage:**

```bash
# Run the baseline zero-shot experiment with hard-coded prompts
python src/main.py --experiment zeroshot_sam_hardcoded --output_dir results/exp_hardcoded

# Run the experiment with LLM-generated text prompts
python src/main.py --experiment zeroshot_sam_llm_text --output_dir results/exp_llm_text

# Fine-tune the SAM model
python src/main.py --experiment finetune_sam --output_dir results/finetuned_model
```

The results of each experiment (metrics, logs, and sample images) will be saved to the specified `--output_dir`.

## Configuration

The prompts used for the LLM-based experiments are stored in `configs/prompts/`. These JSON files are generated by the scripts in `src/` (e.g., `generate_text_prompts.py`).

- `hard_coded_prompts.json`: Simple, manually defined prompts.
- `llm_text_prompts_*.json`: Text prompts generated by different versions of LLM prompting strategies.
- `llm_multimodal_prompts_*.json`: Multimodal prompts generated by different versions of LLM prompting strategies.

## Running on an HPC Cluster (SLURM)

The `run_gpu.slurm` script is configured to run the training/evaluation on a GPU node in a SLURM-based HPC environment.

**To use it:**
1.  Modify the script to set your account details and desired Python environment.
2.  Adjust the `python src/main.py ...` command to run the experiment you want.
3.  Submit the job to the queue:
    ```bash
    sbatch run_gpu.slurm
    ```
The output logs will be saved in a `logs/` directory in your scratch space.

## Dataset Details

This project uses the Breast Cancer Semantic Segmentation (BCSS) dataset from Amgad et al., 2019.

- **Classes**: The masks contain integer pixel values corresponding to different tissue types:
  - `0`: Background
  - `1`: Tumor
  - `2`: Stroma
  - `3`: Lymphocyte
  - `4`: Necrosis
  - `5`: Blood Vessel
- **Data Split**: The `src/dataset.py` script performs a deterministic split into training, validation, and test sets based on the TCGA slide identifiers to ensure that images from the same patient do not cross from the training set into the test set.