# @package _global_

# Optimized configuration for histopathology/BCSS dataset
# Based on best practices for medical imaging and SAM2 finetuning

defaults:
  - base_finetune

experiment:
  name: histology_optimized

scratch:
  resolution: 1024
  train_batch_size: 4               # Small dataset needs smaller batch for more steps
  num_train_workers: 8              
  num_frames: 1
  max_num_objects: 3
  base_lr: 2.0e-05                  # Conservative LR for medical images
  vision_lr: 2.0e-06                # 10x smaller for backbone
  phases_per_epoch: 1
  num_epochs: 150                   # More epochs for small dataset (~3150 steps)

vos:
  train_transforms:
    - _target_: training.dataset.transforms.ComposeAPI
      transforms:
        # Geometric augmentations (critical for histopathology)
        - _target_: training.dataset.transforms.RandomHorizontalFlip
          consistent_transform: true
        - _target_: training.dataset.transforms.RandomVerticalFlip
          consistent_transform: true
        # Rotation is important for histopathology (no natural orientation)
        - _target_: training.dataset.transforms.RandomAffine
          degrees: 90                # 90-degree rotations (tissue has no fixed orientation)
          consistent_transform: true
          scale: [0.9, 1.1]         # Slight scale variation
          translate: [0.05, 0.05]   # Small translation
        - _target_: training.dataset.transforms.RandomResizeAPI
          sizes: ${scratch.resolution}
          square: true
          consistent_transform: true
        # Color augmentation (critical for H&E stain variation)
        # H&E stained slides have significant color/stain variation across labs
        - _target_: training.dataset.transforms.ColorJitter
          brightness: 0.3           # Increased for stain variation
          contrast: 0.3             # Handle different scanning protocols
          saturation: 0.3           # Stain intensity varies
          hue: 0.08                 # Moderate hue shift for H&E variation
          consistent_transform: true
        - _target_: training.dataset.transforms.ToTensorAPI
        - _target_: training.dataset.transforms.NormalizeAPI
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]

trainer:
  optim:
    amp:
      enabled: true
      amp_dtype: bfloat16
    optimizer:
      _target_: torch.optim.AdamW
      weight_decay: 0.05            # Regularization for small dataset
    gradient_clip:
      _target_: training.optimizer.GradientClipper
      max_norm: 0.1                 # Prevent exploding gradients
      norm_type: 2
    param_group_modifiers: []
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CosineParamScheduler
            start_value: ${scratch.base_lr}
            end_value: 1.0e-07      # Decay to very small LR
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05

  checkpoint:
    save_freq: 10                   # Save every 10 epochs (more frequent for monitoring)
