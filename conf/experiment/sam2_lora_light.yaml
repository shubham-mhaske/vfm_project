# @package _global_

# SAM2-only finetuning with MINIMAL adaptation strategy
# Goal: Beat zeroshot (0.517) without destroying pretrained knowledge
# Key differences from v2:
#   1. Much lower LR (10x smaller) - gentle adaptation
#   2. Longer warmup (25% of training) - slow ramp up
#   3. Early stopping at 20 epochs - prevent overfitting
#   4. Focal-weighted loss for class balance
#   5. LoRA-style: only tune mask decoder, keep encoder frozen
#   6. Per-class weighted loss with blood_vessel emphasis

defaults:
  - _self_

experiment:
  name: sam2_lora_light

scratch:
  resolution: 1024
  train_batch_size: 4               # Smaller batch = more updates, better generalization
  num_train_workers: 8
  num_frames: 1
  max_num_objects: 3
  base_lr: 5.0e-06                  # 12x SMALLER than v2 (6e-5) - very gentle adaptation
  vision_lr: 0.0                    # COMPLETELY frozen encoder (no gradient)
  phases_per_epoch: 1
  num_epochs: 30                    # SHORT training - prevent overfitting
  warmup_steps: 300                 # ~25% warmup (batch 4 ≈ 21 steps/epoch, 30 epochs ≈ 630 steps)

vos:
  train_transforms:
    - _target_: training.dataset.transforms.ComposeAPI
      transforms:
        # MINIMAL geometric augmentation - preserve learned features
        - _target_: training.dataset.transforms.RandomHorizontalFlip
          consistent_transform: true
        - _target_: training.dataset.transforms.RandomVerticalFlip
          consistent_transform: true
        # NO rotation - let model use pretrained orientation knowledge
        - _target_: training.dataset.transforms.RandomResizeAPI
          sizes: ${scratch.resolution}
          square: true
          consistent_transform: true
        # VERY MILD color jitter - don't destroy color priors
        - _target_: training.dataset.transforms.ColorJitter
          brightness: 0.1           # HALVED from v2 (0.2)
          contrast: 0.1
          saturation: 0.1
          hue: 0.02                 # MINIMAL hue shift
          consistent_transform: true
        - _target_: training.dataset.transforms.ToTensorAPI
        - _target_: training.dataset.transforms.NormalizeAPI
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]

trainer:
  _target_: training.trainer.Trainer
  mode: train_only
  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}
  accelerator: cuda
  seed_value: 42

  model:
    _target_: training.model.sam2.SAM2Train
    freeze_image_encoder: true      # COMPLETELY frozen - key for preserving zeroshot
    image_encoder:
      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder
      scalp: 1
      trunk:
        _target_: sam2.modeling.backbones.hieradet.Hiera
        embed_dim: 144
        num_heads: 2
        stages: [2, 6, 36, 4]
        global_att_blocks: [23, 33, 43]
        window_pos_embed_bkg_spatial_size: [7, 7]
        window_spec: [8, 4, 16, 8]
      neck:
        _target_: sam2.modeling.backbones.image_encoder.FpnNeck
        position_encoding:
          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
          num_pos_feats: 256
          normalize: true
          scale: null
          temperature: 10000
        d_model: 256
        backbone_channel_list: [1152, 576, 288, 144]
        fpn_top_down_levels: [2, 3]
        fpn_interp_model: nearest

    memory_attention:
      _target_: sam2.modeling.memory_attention.MemoryAttention
      d_model: 256
      pos_enc_at_input: true
      layer:
        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer
        activation: relu
        dim_feedforward: 2048
        dropout: 0.1
        pos_enc_at_attn: false
        self_attention:
          _target_: sam2.modeling.sam.transformer.RoPEAttention
          rope_theta: 10000.0
          feat_sizes: [64, 64]
          embedding_dim: 256
          num_heads: 1
          downsample_rate: 1
          dropout: 0.1
        d_model: 256
        pos_enc_at_cross_attn_keys: true
        pos_enc_at_cross_attn_queries: false
        cross_attention:
          _target_: sam2.modeling.sam.transformer.RoPEAttention
          rope_theta: 10000.0
          feat_sizes: [64, 64]
          rope_k_repeat: true
          embedding_dim: 256
          num_heads: 1
          downsample_rate: 1
          dropout: 0.1
          kv_in_dim: 64
      num_layers: 4

    memory_encoder:
      _target_: sam2.modeling.memory_encoder.MemoryEncoder
      out_dim: 64
      position_encoding:
        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
        num_pos_feats: 64
        normalize: true
        scale: null
        temperature: 10000
      mask_downsampler:
        _target_: sam2.modeling.memory_encoder.MaskDownSampler
        kernel_size: 3
        stride: 2
        padding: 1
      fuser:
        _target_: sam2.modeling.memory_encoder.Fuser
        layer:
          _target_: sam2.modeling.memory_encoder.CXBlock
          dim: 256
          kernel_size: 7
          padding: 3
          layer_scale_init_value: 1.0e-06
          use_dwconv: true
        num_layers: 2

    num_maskmem: 7
    image_size: ${scratch.resolution}
    sigmoid_scale_for_mem_enc: 20.0
    sigmoid_bias_for_mem_enc: -10.0
    use_mask_input_as_output_without_sam: true
    directly_add_no_mem_embed: true
    no_obj_embed_spatial: true
    use_high_res_features_in_sam: true
    multimask_output_in_sam: true
    iou_prediction_use_sigmoid: true
    use_obj_ptrs_in_encoder: true
    add_tpos_enc_to_obj_ptrs: true
    proj_tpos_enc_in_obj_ptrs: true
    use_signed_tpos_enc_to_obj_ptrs: true
    only_obj_ptrs_in_the_past_for_eval: true
    pred_obj_scores: true
    pred_obj_scores_mlp: true
    fixed_no_obj_ptr: true
    multimask_output_for_tracking: true
    use_multimask_token_for_obj_ptr: true
    multimask_min_pt_num: 0
    multimask_max_pt_num: 1
    use_mlp_for_obj_ptr_proj: true
    explicit_prompt_type: ${trainer.data.train.datasets.0.video_dataset.prompt_type}
    prob_to_use_pt_input_for_train: 0.0
    prob_to_use_box_input_for_train: 0.0
    prob_to_sample_from_gt_for_train: 0.0
    num_frames_to_correct_for_train: 1
    num_frames_to_correct_for_eval: 1
    rand_frames_to_correct_for_train: false
    add_all_frames_to_correct_as_cond: false
    num_init_cond_frames_for_train: 1
    rand_init_cond_frames_for_train: false
    num_correction_pt_per_frame: 0

  data:
    train:
      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset
      phases_per_epoch: ${scratch.phases_per_epoch}
      batch_sizes:
        - ${scratch.train_batch_size}
      datasets:
        - _target_: training.dataset.vos_dataset.VOSDataset
          transforms: ${vos.train_transforms}
          training: true
          video_dataset:
            _target_: src.finetune_dataset.BCSSRawDataset
            img_folder: ${data_root}/images
            gt_folder: ${data_root}/masks
            split: train
            prompt_type: mixed
            use_neg_points: true
            num_points: 5
          sampler:
            _target_: training.dataset.vos_sampler.RandomUniformSampler
            num_frames: ${scratch.num_frames}
            max_num_objects: ${scratch.max_num_objects}
          multiplier: 1
      shuffle: true
      num_workers: ${scratch.num_train_workers}
      pin_memory: true
      drop_last: true
      collate_fn:
        _target_: training.utils.data_utils.collate_fn
        _partial_: true
        dict_key: all

  optim:
    amp:
      enabled: true
      amp_dtype: bfloat16
    optimizer:
      _target_: torch.optim.AdamW
    gradient_clip:
      _target_: training.optimizer.GradientClipper
      max_norm: 0.1
      norm_type: 2
    param_group_modifiers: []
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 0.0
                end_value: ${scratch.base_lr}
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${scratch.base_lr}
                end_value: 1.0e-07      # Decay to near-zero
            lengths: [0.25, 0.75]       # 25% warmup (longer for stability)
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.001                # VERY LOW weight decay - don't regularize away pretrained knowledge

  loss:
    all:
      _target_: training.loss_fns.MultiStepMultiMasksAndIous
      weight_dict:
        loss_mask: 2                  # REDUCED mask weight (was 5) - less aggressive
        loss_dice: 5                  # INCREASED dice weight - emphasize shape/boundary
        loss_iou: 2                   # INCREASED IoU weight - better overlap
        loss_class: 1
      supervise_all_iou: true
      iou_use_l1_loss: true
      pred_obj_scores: true

  distributed:
    backend: nccl
    find_unused_parameters: true

  logging:
    tensorboard_writer:
      _target_: training.utils.logger.make_tensorboard_logger
      log_dir: ${hydra:run.dir}/tensorboard
      flush_secs: 60
      should_log: true
    log_dir: ${hydra:run.dir}/logs
    log_freq: 10                      # More frequent logging for short training

  checkpoint:
    save_dir: ${hydra:run.dir}/checkpoints
    save_freq: 2                      # Save every 2 epochs for fine-grained selection
    model_weight_initializer:
      _partial_: true
      _target_: training.utils.checkpoint_utils.load_state_dict_into_model
      strict: false
      state_dict:
        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: sam2/checkpoints/sam2.1_hiera_large.pt
        ckpt_state_dict_keys: [model]
