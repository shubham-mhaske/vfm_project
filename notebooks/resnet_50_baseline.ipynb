{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WBEGICEb_I1",
        "outputId": "568bbec0-e553-46aa-f4f0-db404d70004b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWqGM0cKV5nY",
        "outputId": "67ccc807-bf92-4822-eddc-5322cc8a9531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/VFM Project/Copy of data.zip\n",
            "   creating: /content/data/\n",
            "  inflating: /content/__MACOSX/._data  \n",
            "  inflating: /content/data/.DS_Store  \n",
            "  inflating: /content/__MACOSX/data/._.DS_Store  \n",
            "   creating: /content/data/bcss/\n",
            "  inflating: /content/data/bcss/.DS_Store  \n",
            "  inflating: /content/__MACOSX/data/bcss/._.DS_Store  \n",
            "   creating: /content/data/bcss/images/\n",
            "  inflating: /content/__MACOSX/data/bcss/._images  \n",
            "   creating: /content/data/bcss/masks/\n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A09X-DX1_xmin55379_ymin38394_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1B6-DX1_xmin16266_ymin50634_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1AZ-DX1_xmin50286_ymin17075_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A03U-DX1_xmin17362_ymin25363_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1JG-DX1_xmin15677_ymin69205_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A3XL-DX1_xmin29910_ymin15820_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A4SD-DX1_xmin53807_ymin11871_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0CM-DX1_xmin18562_ymin56852_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0TS-DX1_xmin118843_ymin22812_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LK-DX1_xmin75611_ymin27593_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-HN-A2NL-DX1_xmin11931_ymin5764_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A5D7-DX1_xmin114443_ymin22490_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LS-DX1_xmin39247_ymin47756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XT-DX1_xmin83566_ymin29931_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A13Z-DX1_xmin30985_ymin49337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XY-DX1_xmin76297_ymin35510_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A142-DX1_xmin62151_ymin41337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1XQ-DX1_xmin61261_ymin33317_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A4SE-DX1_xmin50530_ymin11318_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0D2-DX1_xmin96887_ymin22299_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0AT-DX1_xmin31171_ymin35756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P1-DX1_xmin50567_ymin38988_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0B3-DX1_xmin82526_ymin42184_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AQ-DX1_xmin18171_ymin38296_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A5D6-DX1_xmin115108_ymin40554_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A14R-DX1_xmin7751_ymin59308_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P4-DX1_xmin17256_ymin35430_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A14X-DX1_xmin88836_ymin66393_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1JF-DX1_xmin24865_ymin52345_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GI-A2C9-DX1_xmin20882_ymin11843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AQ-A04J-DX1_xmin64631_ymin12074_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AY-DX1_xmin62652_ymin27061_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AR-DX1_xmin12708_ymin29100_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A129-DX1_xmin37502_ymin12501_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A6SB-DX1_xmin34625_ymin56578_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E9-A5FL-DX1_xmin45595_ymin47283_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-S3-AA10-DX1_xmin43039_ymin23986_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A2LH-DX1_xmin78480_ymin42305_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A573-DX1_xmin59666_ymin56141_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A26G-DX1_xmin77919_ymin10728_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A07O-DX1_xmin11777_ymin16227_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0E6-DX1_xmin54786_ymin20002_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A2BK-DX1_xmin15426_ymin8344_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A1EW-DX1_xmin66752_ymin42706_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A6IW-DX1_xmin15387_ymin6222_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A27H-DX1_xmin8738_ymin8063_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A04P-DX1_xmin104246_ymin48517_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A1FC-DX1_xmin52216_ymin33355_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A0DA-DX1_xmin43731_ymin18175_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A6VV-DX1_xmin48674_ymin37475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DF-DX1_xmin50637_ymin43774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A1-A0SP-DX1_xmin6798_ymin53719_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0AR-DX1_xmin8468_ymin21166_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A5RW-DX1_xmin5772_ymin6839_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LL-DX1_xmin5783_ymin22777_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0ST-DX1_xmin109446_ymin18274_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A5YO-DX1_xmin36631_ymin44396_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DB-DX1_xmin50586_ymin43110_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A441-DX1_xmin82006_ymin43121_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0BW-DX1_xmin42346_ymin30843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DD-DX1_xmin47260_ymin22408_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A256-DX1_xmin41577_ymin17076_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A5QQ-DX1_xmin107130_ymin49863_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A04Q-DX1_xmin20868_ymin17804_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1JL-DX1_xmin39765_ymin24835_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1PB-DX1_xmin57214_ymin25940_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A18V-DX1_xmin50030_ymin48454_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DI-DX1_xmin22802_ymin51060_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A27M-DX1_xmin54473_ymin16109_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A159-DX1_xmin38620_ymin28700_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0WA-DX1_xmin56581_ymin24774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A26X-DX1_xmin8990_ymin27544_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0TU-DX1_xmin88352_ymin23098_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A3U0-DX1_xmin113512_ymin48566_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0YM-DX1_xmin44520_ymin66915_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A3M7-DX1_xmin41677_ymin66930_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A740-DX1_xmin39436_ymin24080_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0RX-DX1_xmin69099_ymin22327_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A147-DX1_xmin66394_ymin19298_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AI-DX1_xmin38671_ymin10616_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A7VC-DX1_xmin20658_ymin22410_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AQ-A54N-DX1_xmin32440_ymin26558_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A42U-DX1_xmin72053_ymin88645_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A66P-DX1_xmin30143_ymin20310_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1OV-DX1_xmin126026_ymin65132_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1XK-DX1_xmin26340_ymin50273_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A12F-DX1_xmin16484_ymin29374_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XX-DX1_xmin69942_ymin41560_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A1F6-DX1_xmin53806_ymin49901_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A27F-DX1_xmin98787_ymin6725_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0T0-DX1_xmin72865_ymin59458_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A150-DX1_xmin42083_ymin62113_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A0CE-DX1_xmin58461_ymin18750_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A2QH-DX1_xmin42173_ymin16859_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A158-DX1_xmin56083_ymin25872_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/images/._TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0T2-DX1_xmin63557_ymin56751_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A0J2-DX1_xmin33561_ymin14515_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A1G6-DX1_xmin57706_ymin69514_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0D0-DX1_xmin68482_ymin39071_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A04T-DX1_xmin72145_ymin39078_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0U4-DX1_xmin41141_ymin69786_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-S3-AA15-DX1_xmin55486_ymin28926_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A128-DX1_xmin11924_ymin4831_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A26F-DX1_xmin72969_ymin11489_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0AL-DX1_xmin15698_ymin29806_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P7-DX1_xmin9808_ymin46395_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A07C-DX1_xmin23538_ymin17397_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0G0-DX1_xmin32741_ymin14546_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A26Y-DX1_xmin29748_ymin36769_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A18G-DX1_xmin16831_ymin497_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1OW-DX1_xmin36101_ymin49680_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0E0-DX1_xmin16640_ymin43761_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A73Y-DX1_xmin68117_ymin49445_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0SX-DX1_xmin53791_ymin56683_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A1HJ-DX1_xmin46412_ymin17174_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A27B-DX1_xmin52632_ymin11349_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LH-DX1_xmin22770_ymin24149_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A6VW-DX1_xmin69350_ymin15716_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A1KR-DX1_xmin19591_ymin37667_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A66I-DX1_xmin38433_ymin22957_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0BL-DX1_xmin25219_ymin42847_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A131-DX1_xmin43049_ymin10894_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A6VO-DX1_xmin88416_ymin188485_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A08R-DX1_xmin14220_ymin13782_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A0J6-DX1_xmin40548_ymin11135_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A0J4-DX1_xmin17194_ymin10629_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A6VY-DX1_xmin7308_ymin29955_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0XU-DX1_xmin26118_ymin18441_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0BG-DX1_xmin64019_ymin24975_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A2LR-DX1_xmin46540_ymin31961_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A12V-DX1_xmin11056_ymin54401_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A2QJ-DX1_xmin36620_ymin8672_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0YE-DX1_xmin59206_ymin33505_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1L7-DX1_xmin51554_ymin15424_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1PH-DX1_xmin3455_ymin52049_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LI-DX1_xmin44715_ymin18434_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DH-DX1_xmin50963_ymin56303_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A124-DX1_xmin16436_ymin1748_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P8-DX1_xmin15520_ymin26381_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0AV-DX1_xmin77937_ymin57165_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A143-DX1_xmin52429_ymin19478_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A97C-DX1_xmin68058_ymin32495_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0U1-DX1_xmin63681_ymin24922_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A5ZV-DX1_xmin103009_ymin26581_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0B9-DX1_xmin62727_ymin56697_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A14N-DX1_xmin21383_ymin66838_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A26I-DX1_xmin9689_ymin25668_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A574-DX1_xmin54962_ymin47475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XS-DX1_xmin21421_ymin37486_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XU-DX1_xmin33329_ymin26157_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A09X-DX1_xmin55379_ymin38394_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1B6-DX1_xmin16266_ymin50634_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1AZ-DX1_xmin50286_ymin17075_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A03U-DX1_xmin17362_ymin25363_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1JG-DX1_xmin15677_ymin69205_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A3XL-DX1_xmin29910_ymin15820_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A4SD-DX1_xmin53807_ymin11871_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0CM-DX1_xmin18562_ymin56852_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A2-A0CM-DX1_xmin18562_ymin56852_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0TS-DX1_xmin118843_ymin22812_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LK-DX1_xmin75611_ymin27593_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-HN-A2NL-DX1_xmin11931_ymin5764_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A5D7-DX1_xmin114443_ymin22490_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LS-DX1_xmin39247_ymin47756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XT-DX1_xmin83566_ymin29931_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A13Z-DX1_xmin30985_ymin49337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XY-DX1_xmin76297_ymin35510_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A142-DX1_xmin62151_ymin41337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1XQ-DX1_xmin61261_ymin33317_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A4SE-DX1_xmin50530_ymin11318_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0D2-DX1_xmin96887_ymin22299_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0AT-DX1_xmin31171_ymin35756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P1-DX1_xmin50567_ymin38988_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0B3-DX1_xmin82526_ymin42184_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AQ-DX1_xmin18171_ymin38296_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A5D6-DX1_xmin115108_ymin40554_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A14R-DX1_xmin7751_ymin59308_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P4-DX1_xmin17256_ymin35430_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A14X-DX1_xmin88836_ymin66393_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1JF-DX1_xmin24865_ymin52345_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GI-A2C9-DX1_xmin20882_ymin11843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AQ-A04J-DX1_xmin64631_ymin12074_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AY-DX1_xmin62652_ymin27061_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AR-DX1_xmin12708_ymin29100_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A129-DX1_xmin37502_ymin12501_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A6SB-DX1_xmin34625_ymin56578_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E9-A5FL-DX1_xmin45595_ymin47283_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-S3-AA10-DX1_xmin43039_ymin23986_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A2LH-DX1_xmin78480_ymin42305_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A573-DX1_xmin59666_ymin56141_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A26G-DX1_xmin77919_ymin10728_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A07O-DX1_xmin11777_ymin16227_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0E6-DX1_xmin54786_ymin20002_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A2BK-DX1_xmin15426_ymin8344_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A1EW-DX1_xmin66752_ymin42706_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A6IW-DX1_xmin15387_ymin6222_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A27H-DX1_xmin8738_ymin8063_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A04P-DX1_xmin104246_ymin48517_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A2-A04P-DX1_xmin104246_ymin48517_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A1FC-DX1_xmin52216_ymin33355_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A0DA-DX1_xmin43731_ymin18175_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A6VV-DX1_xmin48674_ymin37475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DF-DX1_xmin50637_ymin43774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A1-A0SP-DX1_xmin6798_ymin53719_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0AR-DX1_xmin8468_ymin21166_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A5RW-DX1_xmin5772_ymin6839_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LL-DX1_xmin5783_ymin22777_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0ST-DX1_xmin109446_ymin18274_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A5YO-DX1_xmin36631_ymin44396_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DB-DX1_xmin50586_ymin43110_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A441-DX1_xmin82006_ymin43121_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0BW-DX1_xmin42346_ymin30843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DD-DX1_xmin47260_ymin22408_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A256-DX1_xmin41577_ymin17076_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A5QQ-DX1_xmin107130_ymin49863_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A04Q-DX1_xmin20868_ymin17804_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1JL-DX1_xmin39765_ymin24835_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1PB-DX1_xmin57214_ymin25940_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A18V-DX1_xmin50030_ymin48454_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DI-DX1_xmin22802_ymin51060_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A27M-DX1_xmin54473_ymin16109_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A159-DX1_xmin38620_ymin28700_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0WA-DX1_xmin56581_ymin24774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A26X-DX1_xmin8990_ymin27544_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0TU-DX1_xmin88352_ymin23098_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A3U0-DX1_xmin113512_ymin48566_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0YM-DX1_xmin44520_ymin66915_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A3M7-DX1_xmin41677_ymin66930_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A740-DX1_xmin39436_ymin24080_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0RX-DX1_xmin69099_ymin22327_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A147-DX1_xmin66394_ymin19298_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AI-DX1_xmin38671_ymin10616_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A7VC-DX1_xmin20658_ymin22410_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AQ-A54N-DX1_xmin32440_ymin26558_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A42U-DX1_xmin72053_ymin88645_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A66P-DX1_xmin30143_ymin20310_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1OV-DX1_xmin126026_ymin65132_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1XK-DX1_xmin26340_ymin50273_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A12F-DX1_xmin16484_ymin29374_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XX-DX1_xmin69942_ymin41560_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A1F6-DX1_xmin53806_ymin49901_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A27F-DX1_xmin98787_ymin6725_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0T0-DX1_xmin72865_ymin59458_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A150-DX1_xmin42083_ymin62113_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A0CE-DX1_xmin58461_ymin18750_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A2QH-DX1_xmin42173_ymin16859_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A158-DX1_xmin56083_ymin25872_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0T2-DX1_xmin63557_ymin56751_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A0J2-DX1_xmin33561_ymin14515_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A1G6-DX1_xmin57706_ymin69514_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0D0-DX1_xmin68482_ymin39071_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A04T-DX1_xmin72145_ymin39078_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A2-A04T-DX1_xmin72145_ymin39078_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0U4-DX1_xmin41141_ymin69786_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-S3-AA15-DX1_xmin55486_ymin28926_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A128-DX1_xmin11924_ymin4831_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A26F-DX1_xmin72969_ymin11489_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0AL-DX1_xmin15698_ymin29806_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P7-DX1_xmin9808_ymin46395_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A07C-DX1_xmin23538_ymin17397_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0G0-DX1_xmin32741_ymin14546_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A26Y-DX1_xmin29748_ymin36769_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A18G-DX1_xmin16831_ymin497_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1OW-DX1_xmin36101_ymin49680_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0E0-DX1_xmin16640_ymin43761_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A73Y-DX1_xmin68117_ymin49445_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0SX-DX1_xmin53791_ymin56683_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A1HJ-DX1_xmin46412_ymin17174_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A27B-DX1_xmin52632_ymin11349_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LH-DX1_xmin22770_ymin24149_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A6VW-DX1_xmin69350_ymin15716_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A1KR-DX1_xmin19591_ymin37667_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A66I-DX1_xmin38433_ymin22957_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0BL-DX1_xmin25219_ymin42847_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A131-DX1_xmin43049_ymin10894_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A6VO-DX1_xmin88416_ymin188485_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A08R-DX1_xmin14220_ymin13782_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A0J6-DX1_xmin40548_ymin11135_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A0J4-DX1_xmin17194_ymin10629_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A6VY-DX1_xmin7308_ymin29955_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0XU-DX1_xmin26118_ymin18441_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0BG-DX1_xmin64019_ymin24975_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A2LR-DX1_xmin46540_ymin31961_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A12V-DX1_xmin11056_ymin54401_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A2QJ-DX1_xmin36620_ymin8672_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0YE-DX1_xmin59206_ymin33505_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1L7-DX1_xmin51554_ymin15424_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1PH-DX1_xmin3455_ymin52049_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LI-DX1_xmin44715_ymin18434_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DH-DX1_xmin50963_ymin56303_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A124-DX1_xmin16436_ymin1748_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P8-DX1_xmin15520_ymin26381_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0AV-DX1_xmin77937_ymin57165_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A143-DX1_xmin52429_ymin19478_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A97C-DX1_xmin68058_ymin32495_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0U1-DX1_xmin63681_ymin24922_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A5ZV-DX1_xmin103009_ymin26581_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0B9-DX1_xmin62727_ymin56697_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A14N-DX1_xmin21383_ymin66838_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-E2-A14N-DX1_xmin21383_ymin66838_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A26I-DX1_xmin9689_ymin25668_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A574-DX1_xmin54962_ymin47475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XS-DX1_xmin21421_ymin37486_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XU-DX1_xmin33329_ymin26157_MPP-0.2500.png  \n"
          ]
        }
      ],
      "source": [
        "# Replace 'My Drive/Path/to/your_data.zip' with the actual path\n",
        "ZIP_PATH = \"/content/drive/MyDrive/VFM Project/Copy of data.zip\"\n",
        "\n",
        "# The unzipped contents will go here (this is fast local storage)\n",
        "EXTRACT_PATH = \"/content/\"\n",
        "\n",
        "! unzip \"$ZIP_PATH\" -d \"$EXTRACT_PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXmW5roQDMiD",
        "outputId": "6fcf3fc7-8f3c-49cb-d06b-db86e4acc5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFDQNpC2DMiD"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/microsoft/resnet-50\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/microsoft/resnet-50)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFKWF7ySDMiE",
        "outputId": "b3fc8974-f7ac-4b9e-bea6-73604b68193b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'macaw', 'score': 0.9999986886978149},\n",
              " {'label': 'vulture', 'score': 2.1576444453330623e-07},\n",
              " {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
              "  'score': 2.3804846094321874e-08},\n",
              " {'label': 'cocktail shaker', 'score': 1.8397171785977662e-08},\n",
              " {'label': 'toucan', 'score': 1.1318117110192816e-08}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-classification\", model=\"microsoft/resnet-50\")\n",
        "pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SuGgJ6f1DMiE"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO7Xsq2fDMiE"
      },
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
        "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I2XCE2hTDMiF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r5_sTVceDMiF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"auto\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "# Using a public image URL instead of a local file that doesn't exist\n",
        "output = client.image_classification(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\", model=\"microsoft/resnet-50\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53205ad9"
      },
      "source": [
        "# Task\n",
        "Implement and train a Mask R-CNN model for instance segmentation on the BCSS dataset. This involves adapting the `BCSSDataset` to output instance masks and bounding boxes, configuring a pre-trained Mask R-CNN model from `torchvision`, and training the model. Finally, visualize sample predictions and provide a summary of the implementation and results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a03309d9"
      },
      "source": [
        "## Inspect and Adapt BCSSDataset for Mask R-CNN\n",
        "\n",
        "### Subtask:\n",
        "Inspect the existing `BCSSDataset` class and modify its `__getitem__` method to extract individual instance masks and bounding boxes for each object within an image, leveraging connected components analysis for each class in the provided mask.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "847b3eb4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `BCSSDataset` class to extract instance masks and bounding boxes. This involves using connected components analysis, which is provided by `cv2.connectedComponentsWithStats` since `cv2` is already imported. I will modify the `__getitem__` method to perform this extraction and format the output for Mask R-CNN training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82d013a"
      },
      "source": [
        "## Install PyTorch and Torchvision\n",
        "\n",
        "### Subtask:\n",
        "Install PyTorch and Torchvision, which provides pre-trained Mask R-CNN models and necessary utilities for object detection. Ensure the correct CUDA-enabled versions are installed if a GPU is available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbb37b1a"
      },
      "source": [
        "**Reasoning**:\n",
        "To install PyTorch and Torchvision, I will use `pip` commands. I will check for CUDA availability and provide the appropriate installation commands for PyTorch, followed by Torchvision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1807f667",
        "outputId": "6a01fe04-eb11-441b-ee96-ccff510a908d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available! Installing PyTorch with CUDA support.\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "PyTorch and Torchvision installation complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available! Installing PyTorch with CUDA support.\")\n",
        "    # Command for CUDA 12.1 (adjust as needed for your CUDA version)\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "else:\n",
        "    print(\"CUDA not available. Installing PyTorch CPU version.\")\n",
        "    # Command for CPU only\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "print(\"PyTorch and Torchvision installation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44694378"
      },
      "source": [
        "## Refine BCSSDataset for Instance Segmentation Data\n",
        "\n",
        "### Subtask:\n",
        "Modify the `BCSSDataset` class to output instance segmentation data (bounding boxes, masks, labels) and implement a transformation pipeline for resizing, padding, and augmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28597836"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the transformation pipeline for instance segmentation, including `DetectionCompose`, `ResizeAndPadForDetection`, and `RandomHorizontalFlipForDetection` classes. Then, I will update the `BCSSDataset` class by modifying its `__init__` and `__getitem__` methods to incorporate these transformations and extract instance-level data (bounding boxes, masks, and labels) using connected components analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "183bcb89"
      },
      "source": [
        "## Load and Modify ResNet50 Model\n",
        "\n",
        "### Subtask:\n",
        "Load a pre-trained ResNet50 model from `torchvision.models` and replace its final classification layer for the BCSS dataset (5 classes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf4a39c"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the pre-trained ResNet50 model and modify its final classification layer for 5 classes, I will use `torchvision.models` to get the ResNet50 model with default weights, identify the input features of the final layer, replace it with a new `nn.Linear` layer with 5 output features, and then move the model to the appropriate device.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88710b86",
        "outputId": "9fb4fd4f-d8a9-42cb-edaa-a065f2e0664a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50 model loaded and modified for 5 classes. Model moved to cuda.\n",
            "New final layer: Linear(in_features=2048, out_features=5, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# 1. Load a pre-trained ResNet50 model\n",
        "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# 2. Access the model's final fully connected layer (fc)\n",
        "# 3. Determine the number of input features to this final layer\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "# 4. Create a new torch.nn.Linear layer with 5 output features\n",
        "# (5 classes: tumor, stroma, lymphocyte, necrosis, blood_vessel)\n",
        "num_classes = 5 # Excluding background, which is class 0 and not an instance\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# 5. Move the modified model to the appropriate device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"ResNet50 model loaded and modified for {num_classes} classes. Model moved to {device}.\")\n",
        "print(f\"New final layer: {model.fc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63537d51"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block executed successfully, indicating that the ResNet50 model was loaded and its final layer was modified as intended. The next logical step is to define the optimizer and the loss function for training this classification model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5e1d407",
        "outputId": "93ed44f3-6fd6-4c71-8b96-7f1c85ece8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer and loss function defined.\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss for classification)\n",
        "# Assuming class labels are 1-indexed (1 to 5) as per the dataset description,\n",
        "# or adjust if they are 0-indexed.\n",
        "# If labels are 1-indexed, they might need to be shifted by -1 for CrossEntropyLoss\n",
        "# which expects class indices from 0 to num_classes - 1.\n",
        "# Let's confirm the labels generated by the BCSSDataset first. Currently, num_classes is 5.\n",
        "# The instance labels in BCSSDataset are 1: 'tumor', 2: 'stroma', 3: 'lymphocyte', 4: 'necrosis', 5: 'blood_vessel'\n",
        "# So, CrossEntropyLoss expects target labels to be in range [0, 4]. We need to subtract 1 from the labels.\n",
        "class_labels_start_idx = 1 # The smallest class_id is 1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Optimizer and loss function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26ad560b",
        "outputId": "97be4b6f-a5c5-4e4e-ddb3-5b1acd8adfe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders re-initialized with custom collate_fn, transformations, and num_workers=0.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms.functional as F_vision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Helper for transformations\n",
        "class DetectionCompose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target, mask):\n",
        "        for t in self.transforms:\n",
        "            image, target, mask = t(image, target, mask)\n",
        "        return image, target, mask\n",
        "\n",
        "class ResizeAndPadForDetection:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, image, target, mask):\n",
        "        # Convert PIL Image to numpy array for easier manipulation\n",
        "        image_np = np.array(image)\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "        h, w = image_np.shape[:2]\n",
        "        scale = self.size / max(h, w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # Resize image and mask\n",
        "        resized_image = cv2.resize(image_np, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "        resized_mask = cv2.resize(mask_np, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Create a new padded image and mask\n",
        "        padded_image = np.zeros((self.size, self.size, 3), dtype=resized_image.dtype)\n",
        "        padded_mask = np.zeros((self.size, self.size), dtype=resized_mask.dtype)\n",
        "\n",
        "        # Place the resized image and mask in the center\n",
        "        start_h = (self.size - new_h) // 2\n",
        "        start_w = (self.size - new_w) // 2\n",
        "        padded_image[start_h : start_h + new_h, start_w : start_w + new_w] = resized_image\n",
        "        padded_mask[start_h : start_h + new_h, start_w : start_w + new_w] = resized_mask\n",
        "\n",
        "        # Adjust bounding boxes\n",
        "        boxes = target['boxes']\n",
        "        if boxes.numel() > 0:\n",
        "            boxes_np = boxes.cpu().numpy()\n",
        "            boxes_np[:, [0, 2]] = boxes_np[:, [0, 2]] * scale + start_w  # xmin, xmax\n",
        "            boxes_np[:, [1, 3]] = boxes_np[:, [1, 3]] * scale + start_h  # ymin, ymax\n",
        "            target['boxes'] = torch.from_numpy(boxes_np).float()\n",
        "\n",
        "        # Adjust masks (instance masks)\n",
        "        instance_masks = target['masks']\n",
        "        if instance_masks.numel() > 0:\n",
        "            # Resize each instance mask\n",
        "            resized_instance_masks = []\n",
        "            for i in range(instance_masks.shape[0]):\n",
        "                mask_i_np = instance_masks[i].cpu().numpy()\n",
        "                resized_mask_i = cv2.resize(mask_i_np.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "                padded_mask_i = np.zeros((self.size, self.size), dtype=np.uint8)\n",
        "                padded_mask_i[start_h : start_h + new_h, start_w : start_w + new_w] = resized_mask_i\n",
        "                resized_instance_masks.append(torch.from_numpy(padded_mask_i))\n",
        "            target['masks'] = torch.stack(resized_instance_masks)\n",
        "\n",
        "        image = F_vision.to_tensor(padded_image.astype(np.float32) / 255.0) # Normalize to [0,1]\n",
        "\n",
        "        return image, target, torch.from_numpy(padded_mask).long()\n",
        "\n",
        "class RandomHorizontalFlipForDetection:\n",
        "    def __init__(self, prob=0.5):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target, mask):\n",
        "        if torch.rand(1) < self.prob:\n",
        "            image = F_vision.hflip(image)\n",
        "            mask = F_vision.hflip(mask.unsqueeze(0)).squeeze(0) # Apply to semantic mask\n",
        "\n",
        "            # Flip bounding boxes\n",
        "            boxes = target['boxes']\n",
        "            if boxes.numel() > 0:\n",
        "                image_width = image.shape[2] # Image is CxHxW\n",
        "                xmin = boxes[:, 0]\n",
        "                xmax = boxes[:, 2]\n",
        "                new_xmin = image_width - xmax\n",
        "                new_xmax = image_width - xmin\n",
        "                boxes[:, 0] = new_xmin\n",
        "                boxes[:, 2] = new_xmax\n",
        "                target['boxes'] = boxes\n",
        "\n",
        "            # Flip instance masks\n",
        "            instance_masks = target['masks']\n",
        "            if instance_masks.numel() > 0:\n",
        "                target['masks'] = F_vision.hflip(instance_masks) # Masks are NxHxW\n",
        "\n",
        "        return image, target, mask\n",
        "\n",
        "def get_transform(train, img_size):\n",
        "    transforms = []\n",
        "    transforms.append(ResizeAndPadForDetection(img_size))\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlipForDetection(0.5))\n",
        "    return DetectionCompose(transforms)\n",
        "\n",
        "\n",
        "class BCSSDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, split='train'):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        all_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
        "\n",
        "        test_prefixes = ['TCGA-OL-', 'TCGA-LL-', 'TCGA-E2-', 'TCGA-EW-', 'TCGA-GM-', 'TCGA-S3-']\n",
        "\n",
        "        test_files = [f for f in all_files if any(f.startswith(p) for p in test_prefixes)]\n",
        "        train_val_files = [f for f in all_files if f not in test_files]\n",
        "\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(train_val_files)\n",
        "        val_split = int(0.2 * len(train_val_files))\n",
        "        val_files = train_val_files[:val_split]\n",
        "        train_files = train_val_files[val_split:]\n",
        "\n",
        "        if split == 'train':\n",
        "            initial_image_files = train_files\n",
        "        elif split == 'val':\n",
        "            initial_image_files = val_files\n",
        "        elif split == 'test':\n",
        "            initial_image_files = test_files\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split: {split}. Must be 'train', 'val', or 'test'.\")\n",
        "\n",
        "        self.class_names = {\n",
        "            0: 'background',\n",
        "            1: 'tumor',\n",
        "            2: 'stroma',\n",
        "            3: 'lymphocyte',\n",
        "            4: 'necrosis',\n",
        "            5: 'blood_vessel'\n",
        "        }\n",
        "\n",
        "        # Filter out images that have no instances (only background in mask)\n",
        "        self.image_files = []\n",
        "        for filename in initial_image_files:\n",
        "            mask_path = os.path.join(self.mask_dir, filename)\n",
        "            mask = np.array(Image.open(mask_path))\n",
        "            unique_classes_in_mask = np.unique(mask)\n",
        "            unique_classes_in_mask = unique_classes_in_mask[unique_classes_in_mask != 0] # Exclude background\n",
        "            if len(unique_classes_in_mask) > 0:\n",
        "                self.image_files.append(filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        mask_path = os.path.join(self.mask_dir, self.image_files[idx])\n",
        "        mask = np.array(Image.open(mask_path)) # Semantic mask\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        instance_masks = []\n",
        "\n",
        "        unique_classes_in_mask = np.unique(mask)\n",
        "        unique_classes_in_mask = unique_classes_in_mask[unique_classes_in_mask != 0] # Exclude background\n",
        "\n",
        "        # Since we filtered out images with no instances in __init__,\n",
        "        # unique_classes_in_mask will always have length > 0 here.\n",
        "\n",
        "        for class_id in unique_classes_in_mask:\n",
        "            # Create a binary mask for the current class\n",
        "            class_binary_mask = (mask == class_id).astype(np.uint8)\n",
        "\n",
        "            # Find connected components for instance segmentation\n",
        "            num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(class_binary_mask, 8, cv2.CV_32S)\n",
        "\n",
        "            for i in range(1, num_labels): # Iterate through instances, skipping background (label 0)\n",
        "                xmin = stats[i, cv2.CC_STAT_LEFT]\n",
        "                ymin = stats[i, cv2.CC_STAT_TOP]\n",
        "                width = stats[i, cv2.CC_STAT_WIDTH]\n",
        "                height = stats[i, cv2.CC_STAT_HEIGHT]\n",
        "                xmax = xmin + width\n",
        "                ymax = ymin + height\n",
        "\n",
        "                # Ensure valid bounding box\n",
        "                if width > 0 and height > 0:\n",
        "                    boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    labels.append(class_id) # Assign the semantic class_id as instance label\n",
        "\n",
        "                    # Create instance mask for the current component\n",
        "                    instance_mask = (labels_img == i).astype(np.uint8)\n",
        "                    instance_masks.append(instance_mask)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        # After filtering in __init__, boxes should ideally never be empty here.\n",
        "        # Keeping the check as a safeguard, although it should rarely be triggered.\n",
        "        if len(boxes) == 0:\n",
        "            print(f\"Warning: Image {self.image_files[idx]} still produced no instances after filtering in __init__.\")\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            instance_masks = torch.zeros((0, mask.shape[0], mask.shape[1]), dtype=torch.uint8)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "            instance_masks = torch.as_tensor(np.array(instance_masks), dtype=torch.uint8)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'masks': instance_masks\n",
        "        }\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image_pil = image # Keep image as PIL for initial transform input\n",
        "            image, target, mask = self.transform(image_pil, target, mask)\n",
        "        else:\n",
        "            image = F_vision.to_tensor(np.array(image).astype(np.float32) / 255.0)\n",
        "            mask = torch.from_numpy(mask).long() # Ensure mask is long for semantic segmentation\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'target': target, # Now contains boxes, labels, and instance_masks\n",
        "            'unique_classes': torch.as_tensor(unique_classes_in_mask, dtype=torch.int64),\n",
        "            'filename': self.image_files[idx]\n",
        "        }\n",
        "\n",
        "def instance_collate_fn(batch):\n",
        "    all_cropped_images = []\n",
        "    all_instance_labels = []\n",
        "\n",
        "    # Define a maximum number of instances to process per batch (which is per image since DataLoader batch_size is 1)\n",
        "    max_instances_per_batch = 64 # Conservative limit to ensure memory fit\n",
        "\n",
        "    for item in batch:\n",
        "        image = item['image'] # C, H, W tensor, already normalized\n",
        "        boxes = item['target']['boxes'] # N, 4 tensor of xmin, ymin, xmax, ymax\n",
        "        labels = item['target']['labels'] # N tensor of class labels\n",
        "\n",
        "        # If there are more instances than allowed, sample a subset\n",
        "        if boxes.shape[0] > max_instances_per_batch:\n",
        "            indices = torch.randperm(boxes.shape[0])[:max_instances_per_batch]\n",
        "            boxes = boxes[indices]\n",
        "            labels = labels[indices]\n",
        "\n",
        "        # Iterate through each instance in the current image (or sampled subset)\n",
        "        for i in range(boxes.shape[0]):\n",
        "            box = boxes[i]\n",
        "            label = labels[i]\n",
        "\n",
        "            # Extract coordinates for cropping (xmin, ymin, xmax, ymax)\n",
        "            xmin, ymin, xmax, ymax = box.int().tolist()\n",
        "\n",
        "            # Calculate height and width for F_vision.crop\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "\n",
        "            # Ensure valid crop dimensions\n",
        "            if width > 0 and height > 0:\n",
        "                # Crop the instance from the image\n",
        "                cropped_image = F_vision.crop(image, ymin, xmin, height, width)\n",
        "\n",
        "                all_cropped_images.append(cropped_image)\n",
        "                all_instance_labels.append(label)\n",
        "\n",
        "    # If no instances were found in the entire batch, return empty tensors\n",
        "    # This case should ideally not be reached if BCSSDataset filters correctly.\n",
        "    if not all_cropped_images:\n",
        "        return torch.empty((0, 3, 128, 128)), torch.empty((0,), dtype=torch.int64) # Updated size\n",
        "\n",
        "    # For simplicity, we'll resize each cropped image to a fixed size (e.g., 128x128)\n",
        "    # to make them stackable into a single batch tensor for a potential classifier.\n",
        "    target_instance_size = (128, 128) # Updated target instance size\n",
        "    processed_cropped_images = []\n",
        "    for img_tensor in all_cropped_images:\n",
        "        # Ensure image is 3 channels, resize using bicubic interpolation\n",
        "        if img_tensor.shape[0] == 1: # If grayscale, convert to 3 channels by repeating\n",
        "            img_tensor = img_tensor.repeat(3, 1, 1)\n",
        "        elif img_tensor.shape[0] == 4: # If RGBA, convert to RGB\n",
        "            img_tensor = img_tensor[:3, :, :]\n",
        "\n",
        "        # Resize to target_instance_size\n",
        "        resized_img = F_vision.resize(img_tensor, target_instance_size, interpolation=F_vision.InterpolationMode.BICUBIC)\n",
        "        processed_cropped_images.append(resized_img)\n",
        "\n",
        "    stacked_cropped_images = torch.stack(processed_cropped_images)\n",
        "    stacked_instance_labels = torch.stack(all_instance_labels)\n",
        "\n",
        "    return stacked_cropped_images, stacked_instance_labels\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "images_path = '/content/data/bcss/images'\n",
        "masks_path = '/content/data/bcss/masks'\n",
        "\n",
        "train_dataset = BCSSDataset(images_path, masks_path, transform=get_transform(train=True, img_size=IMG_SIZE), split='train')\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=instance_collate_fn)\n",
        "\n",
        "val_dataset = BCSSDataset(images_path, masks_path, transform=get_transform(train=False, img_size=IMG_SIZE), split='val')\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=0, collate_fn=instance_collate_fn)\n",
        "\n",
        "test_dataset = BCSSDataset(images_path, masks_path, transform=get_transform(train=False, img_size=IMG_SIZE), split='test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=0, collate_fn=instance_collate_fn)\n",
        "\n",
        "print(\"DataLoaders re-initialized with custom collate_fn, transformations, and num_workers=0.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db6dbdb"
      },
      "source": [
        "**Reasoning**:\n",
        "The DataLoaders have been re-initialized with `num_workers=0` to address the previous `RuntimeError`. Now, the next step is to re-execute the `train_model` function to proceed with training and determine if the worker error is resolved or if other issues arise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "57c71786",
        "outputId": "b7ec2630-2232-4d0f-db3e-99bd154a0616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1977997876.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Call the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels_start_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1977997876.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, class_labels_start_idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                     new_grads.append(\n\u001b[0;32m--> 221\u001b[0;31m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                     )\n\u001b[1;32m    223\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# Set environment variable to avoid CUDA memory fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "# Set CUDA_LAUNCH_BLOCKING to 1 for more detailed error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, class_labels_start_idx):\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            # Skip empty batches if instance_collate_fn returns them\n",
        "            if inputs.numel() == 0:\n",
        "                print(f\"  Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}]: Skipping empty batch.\")\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            # Adjust labels for CrossEntropyLoss (0-indexed)\n",
        "            labels = (labels - class_labels_start_idx).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0: # Print every 10 batches\n",
        "                print(f\"  Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_duration = end_time - start_time\n",
        "        print(f\"Epoch {epoch+1} finished. Avg Loss: {running_loss/len(train_loader):.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # Skip empty batches during validation\n",
        "                if inputs.numel() == 0:\n",
        "                    print(f\"  Validation: Skipping empty batch.\")\n",
        "                    continue\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                labels = (labels - class_labels_start_idx).to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "# Call the training function\n",
        "num_epochs = 10\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, class_labels_start_idx)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}