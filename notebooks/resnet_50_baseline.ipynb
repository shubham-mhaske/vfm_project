{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WBEGICEb_I1",
        "outputId": "a589d7db-08f6-4bce-890d-e063a72cab8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWqGM0cKV5nY",
        "outputId": "6497ae37-02b3-4cb6-affe-56ed3df046e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/VFM Project/Copy of data.zip\n",
            "   creating: /content/data/\n",
            "  inflating: /content/__MACOSX/._data  \n",
            "  inflating: /content/data/.DS_Store  \n",
            "  inflating: /content/__MACOSX/data/._.DS_Store  \n",
            "   creating: /content/data/bcss/\n",
            "  inflating: /content/data/bcss/.DS_Store  \n",
            "  inflating: /content/__MACOSX/data/bcss/._.DS_Store  \n",
            "   creating: /content/data/bcss/images/\n",
            "  inflating: /content/__MACOSX/data/bcss/._images  \n",
            "   creating: /content/data/bcss/masks/\n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A09X-DX1_xmin55379_ymin38394_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1B6-DX1_xmin16266_ymin50634_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1AZ-DX1_xmin50286_ymin17075_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A03U-DX1_xmin17362_ymin25363_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1JG-DX1_xmin15677_ymin69205_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A3XL-DX1_xmin29910_ymin15820_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A4SD-DX1_xmin53807_ymin11871_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0CM-DX1_xmin18562_ymin56852_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0TS-DX1_xmin118843_ymin22812_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LK-DX1_xmin75611_ymin27593_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-HN-A2NL-DX1_xmin11931_ymin5764_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A5D7-DX1_xmin114443_ymin22490_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LS-DX1_xmin39247_ymin47756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XT-DX1_xmin83566_ymin29931_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A13Z-DX1_xmin30985_ymin49337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XY-DX1_xmin76297_ymin35510_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A142-DX1_xmin62151_ymin41337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1XQ-DX1_xmin61261_ymin33317_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A4SE-DX1_xmin50530_ymin11318_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0D2-DX1_xmin96887_ymin22299_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0AT-DX1_xmin31171_ymin35756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P1-DX1_xmin50567_ymin38988_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0B3-DX1_xmin82526_ymin42184_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AQ-DX1_xmin18171_ymin38296_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A5D6-DX1_xmin115108_ymin40554_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A14R-DX1_xmin7751_ymin59308_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P4-DX1_xmin17256_ymin35430_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A14X-DX1_xmin88836_ymin66393_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1JF-DX1_xmin24865_ymin52345_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GI-A2C9-DX1_xmin20882_ymin11843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AQ-A04J-DX1_xmin64631_ymin12074_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AY-DX1_xmin62652_ymin27061_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AR-DX1_xmin12708_ymin29100_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A129-DX1_xmin37502_ymin12501_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A6SB-DX1_xmin34625_ymin56578_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E9-A5FL-DX1_xmin45595_ymin47283_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-S3-AA10-DX1_xmin43039_ymin23986_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A2LH-DX1_xmin78480_ymin42305_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A573-DX1_xmin59666_ymin56141_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A26G-DX1_xmin77919_ymin10728_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A07O-DX1_xmin11777_ymin16227_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0E6-DX1_xmin54786_ymin20002_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A2BK-DX1_xmin15426_ymin8344_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A1EW-DX1_xmin66752_ymin42706_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A6IW-DX1_xmin15387_ymin6222_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A27H-DX1_xmin8738_ymin8063_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A04P-DX1_xmin104246_ymin48517_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A1FC-DX1_xmin52216_ymin33355_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A0DA-DX1_xmin43731_ymin18175_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A6VV-DX1_xmin48674_ymin37475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DF-DX1_xmin50637_ymin43774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A1-A0SP-DX1_xmin6798_ymin53719_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0AR-DX1_xmin8468_ymin21166_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A5RW-DX1_xmin5772_ymin6839_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LL-DX1_xmin5783_ymin22777_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0ST-DX1_xmin109446_ymin18274_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A5YO-DX1_xmin36631_ymin44396_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DB-DX1_xmin50586_ymin43110_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A441-DX1_xmin82006_ymin43121_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0BW-DX1_xmin42346_ymin30843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DD-DX1_xmin47260_ymin22408_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A256-DX1_xmin41577_ymin17076_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A5QQ-DX1_xmin107130_ymin49863_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A04Q-DX1_xmin20868_ymin17804_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1JL-DX1_xmin39765_ymin24835_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1PB-DX1_xmin57214_ymin25940_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A18V-DX1_xmin50030_ymin48454_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DI-DX1_xmin22802_ymin51060_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A27M-DX1_xmin54473_ymin16109_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A159-DX1_xmin38620_ymin28700_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0WA-DX1_xmin56581_ymin24774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A26X-DX1_xmin8990_ymin27544_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0TU-DX1_xmin88352_ymin23098_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A3U0-DX1_xmin113512_ymin48566_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0YM-DX1_xmin44520_ymin66915_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A3M7-DX1_xmin41677_ymin66930_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A740-DX1_xmin39436_ymin24080_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0RX-DX1_xmin69099_ymin22327_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A147-DX1_xmin66394_ymin19298_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A1AI-DX1_xmin38671_ymin10616_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A7VC-DX1_xmin20658_ymin22410_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AQ-A54N-DX1_xmin32440_ymin26558_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A42U-DX1_xmin72053_ymin88645_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A66P-DX1_xmin30143_ymin20310_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1OV-DX1_xmin126026_ymin65132_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A1XK-DX1_xmin26340_ymin50273_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A12F-DX1_xmin16484_ymin29374_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XX-DX1_xmin69942_ymin41560_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A1F6-DX1_xmin53806_ymin49901_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A27F-DX1_xmin98787_ymin6725_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0T0-DX1_xmin72865_ymin59458_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A150-DX1_xmin42083_ymin62113_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A0CE-DX1_xmin58461_ymin18750_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A2QH-DX1_xmin42173_ymin16859_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A158-DX1_xmin56083_ymin25872_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/images/._TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0T2-DX1_xmin63557_ymin56751_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A0J2-DX1_xmin33561_ymin14515_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A1G6-DX1_xmin57706_ymin69514_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0D0-DX1_xmin68482_ymin39071_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A04T-DX1_xmin72145_ymin39078_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0U4-DX1_xmin41141_ymin69786_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-S3-AA15-DX1_xmin55486_ymin28926_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A128-DX1_xmin11924_ymin4831_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A26F-DX1_xmin72969_ymin11489_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0AL-DX1_xmin15698_ymin29806_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P7-DX1_xmin9808_ymin46395_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A07C-DX1_xmin23538_ymin17397_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0G0-DX1_xmin32741_ymin14546_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A26Y-DX1_xmin29748_ymin36769_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A18G-DX1_xmin16831_ymin497_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1OW-DX1_xmin36101_ymin49680_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0E0-DX1_xmin16640_ymin43761_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-LL-A73Y-DX1_xmin68117_ymin49445_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0SX-DX1_xmin53791_ymin56683_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A1HJ-DX1_xmin46412_ymin17174_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A27B-DX1_xmin52632_ymin11349_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LH-DX1_xmin22770_ymin24149_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A6VW-DX1_xmin69350_ymin15716_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A1KR-DX1_xmin19591_ymin37667_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A66I-DX1_xmin38433_ymin22957_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0BL-DX1_xmin25219_ymin42847_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A131-DX1_xmin43049_ymin10894_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A6VO-DX1_xmin88416_ymin188485_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A8-A08R-DX1_xmin14220_ymin13782_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A0J6-DX1_xmin40548_ymin11135_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A0J4-DX1_xmin17194_ymin10629_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A6VY-DX1_xmin7308_ymin29955_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AN-A0XU-DX1_xmin26118_ymin18441_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0BG-DX1_xmin64019_ymin24975_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A2LR-DX1_xmin46540_ymin31961_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-C8-A12V-DX1_xmin11056_ymin54401_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AC-A2QJ-DX1_xmin36620_ymin8672_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A0YE-DX1_xmin59206_ymin33505_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1L7-DX1_xmin51554_ymin15424_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1PH-DX1_xmin3455_ymin52049_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A1LI-DX1_xmin44715_ymin18434_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-GM-A2DH-DX1_xmin50963_ymin56303_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AO-A124-DX1_xmin16436_ymin1748_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-EW-A1P8-DX1_xmin15520_ymin26381_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0AV-DX1_xmin77937_ymin57165_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-D8-A143-DX1_xmin52429_ymin19478_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-OL-A97C-DX1_xmin68058_ymin32495_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-AR-A0U1-DX1_xmin63681_ymin24922_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A5ZV-DX1_xmin103009_ymin26581_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-BH-A0B9-DX1_xmin62727_ymin56697_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A14N-DX1_xmin21383_ymin66838_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A7-A26I-DX1_xmin9689_ymin25668_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-E2-A574-DX1_xmin54962_ymin47475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XS-DX1_xmin21421_ymin37486_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/images/TCGA-A2-A3XU-DX1_xmin33329_ymin26157_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A09X-DX1_xmin55379_ymin38394_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1B6-DX1_xmin16266_ymin50634_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1AZ-DX1_xmin50286_ymin17075_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A03U-DX1_xmin17362_ymin25363_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1JG-DX1_xmin15677_ymin69205_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A3XL-DX1_xmin29910_ymin15820_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A4SD-DX1_xmin53807_ymin11871_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0CM-DX1_xmin18562_ymin56852_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A2-A0CM-DX1_xmin18562_ymin56852_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0TS-DX1_xmin118843_ymin22812_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LK-DX1_xmin75611_ymin27593_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-HN-A2NL-DX1_xmin11931_ymin5764_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A5D7-DX1_xmin114443_ymin22490_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LS-DX1_xmin39247_ymin47756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XT-DX1_xmin83566_ymin29931_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A13Z-DX1_xmin30985_ymin49337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XY-DX1_xmin76297_ymin35510_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A142-DX1_xmin62151_ymin41337_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1XQ-DX1_xmin61261_ymin33317_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A4SE-DX1_xmin50530_ymin11318_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0D2-DX1_xmin96887_ymin22299_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0AT-DX1_xmin31171_ymin35756_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P1-DX1_xmin50567_ymin38988_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0B3-DX1_xmin82526_ymin42184_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AQ-DX1_xmin18171_ymin38296_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A5D6-DX1_xmin115108_ymin40554_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A14R-DX1_xmin7751_ymin59308_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P4-DX1_xmin17256_ymin35430_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A14X-DX1_xmin88836_ymin66393_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1JF-DX1_xmin24865_ymin52345_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GI-A2C9-DX1_xmin20882_ymin11843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AQ-A04J-DX1_xmin64631_ymin12074_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AY-DX1_xmin62652_ymin27061_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AR-DX1_xmin12708_ymin29100_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A129-DX1_xmin37502_ymin12501_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A6SB-DX1_xmin34625_ymin56578_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E9-A5FL-DX1_xmin45595_ymin47283_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-S3-AA10-DX1_xmin43039_ymin23986_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A2LH-DX1_xmin78480_ymin42305_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A573-DX1_xmin59666_ymin56141_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A26G-DX1_xmin77919_ymin10728_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A07O-DX1_xmin11777_ymin16227_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0E6-DX1_xmin54786_ymin20002_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A2BK-DX1_xmin15426_ymin8344_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A1EW-DX1_xmin66752_ymin42706_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A6IW-DX1_xmin15387_ymin6222_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A27H-DX1_xmin8738_ymin8063_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A04P-DX1_xmin104246_ymin48517_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A2-A04P-DX1_xmin104246_ymin48517_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A1FC-DX1_xmin52216_ymin33355_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A0DA-DX1_xmin43731_ymin18175_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A6VV-DX1_xmin48674_ymin37475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DF-DX1_xmin50637_ymin43774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A1-A0SP-DX1_xmin6798_ymin53719_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0AR-DX1_xmin8468_ymin21166_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A5RW-DX1_xmin5772_ymin6839_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LL-DX1_xmin5783_ymin22777_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0ST-DX1_xmin109446_ymin18274_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A5YO-DX1_xmin36631_ymin44396_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DB-DX1_xmin50586_ymin43110_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A441-DX1_xmin82006_ymin43121_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0BW-DX1_xmin42346_ymin30843_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DD-DX1_xmin47260_ymin22408_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A256-DX1_xmin41577_ymin17076_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A5QQ-DX1_xmin107130_ymin49863_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A04Q-DX1_xmin20868_ymin17804_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1JL-DX1_xmin39765_ymin24835_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1PB-DX1_xmin57214_ymin25940_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A18V-DX1_xmin50030_ymin48454_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DI-DX1_xmin22802_ymin51060_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A27M-DX1_xmin54473_ymin16109_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A159-DX1_xmin38620_ymin28700_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0WA-DX1_xmin56581_ymin24774_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A26X-DX1_xmin8990_ymin27544_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0TU-DX1_xmin88352_ymin23098_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A3U0-DX1_xmin113512_ymin48566_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0YM-DX1_xmin44520_ymin66915_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A3M7-DX1_xmin41677_ymin66930_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A740-DX1_xmin39436_ymin24080_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0RX-DX1_xmin69099_ymin22327_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A147-DX1_xmin66394_ymin19298_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A1AI-DX1_xmin38671_ymin10616_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A7VC-DX1_xmin20658_ymin22410_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AQ-A54N-DX1_xmin32440_ymin26558_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A42U-DX1_xmin72053_ymin88645_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A66P-DX1_xmin30143_ymin20310_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1OV-DX1_xmin126026_ymin65132_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A1XK-DX1_xmin26340_ymin50273_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A12F-DX1_xmin16484_ymin29374_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XX-DX1_xmin69942_ymin41560_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A1F6-DX1_xmin53806_ymin49901_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A27F-DX1_xmin98787_ymin6725_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0T0-DX1_xmin72865_ymin59458_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A150-DX1_xmin42083_ymin62113_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A0CE-DX1_xmin58461_ymin18750_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A2QH-DX1_xmin42173_ymin16859_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A158-DX1_xmin56083_ymin25872_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0T2-DX1_xmin63557_ymin56751_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A0J2-DX1_xmin33561_ymin14515_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A1G6-DX1_xmin57706_ymin69514_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0D0-DX1_xmin68482_ymin39071_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A04T-DX1_xmin72145_ymin39078_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-A2-A04T-DX1_xmin72145_ymin39078_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0U4-DX1_xmin41141_ymin69786_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-S3-AA15-DX1_xmin55486_ymin28926_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A128-DX1_xmin11924_ymin4831_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A26F-DX1_xmin72969_ymin11489_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0AL-DX1_xmin15698_ymin29806_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P7-DX1_xmin9808_ymin46395_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A07C-DX1_xmin23538_ymin17397_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0G0-DX1_xmin32741_ymin14546_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A26Y-DX1_xmin29748_ymin36769_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A18G-DX1_xmin16831_ymin497_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1OW-DX1_xmin36101_ymin49680_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0E0-DX1_xmin16640_ymin43761_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-LL-A73Y-DX1_xmin68117_ymin49445_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0SX-DX1_xmin53791_ymin56683_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A1HJ-DX1_xmin46412_ymin17174_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A27B-DX1_xmin52632_ymin11349_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LH-DX1_xmin22770_ymin24149_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A6VW-DX1_xmin69350_ymin15716_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A1KR-DX1_xmin19591_ymin37667_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A66I-DX1_xmin38433_ymin22957_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0BL-DX1_xmin25219_ymin42847_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A131-DX1_xmin43049_ymin10894_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A6VO-DX1_xmin88416_ymin188485_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A8-A08R-DX1_xmin14220_ymin13782_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A0J6-DX1_xmin40548_ymin11135_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A0J4-DX1_xmin17194_ymin10629_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A6VY-DX1_xmin7308_ymin29955_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AN-A0XU-DX1_xmin26118_ymin18441_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0BG-DX1_xmin64019_ymin24975_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A2LR-DX1_xmin46540_ymin31961_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-C8-A12V-DX1_xmin11056_ymin54401_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AC-A2QJ-DX1_xmin36620_ymin8672_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A0YE-DX1_xmin59206_ymin33505_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1L7-DX1_xmin51554_ymin15424_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1PH-DX1_xmin3455_ymin52049_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A1LI-DX1_xmin44715_ymin18434_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-GM-A2DH-DX1_xmin50963_ymin56303_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AO-A124-DX1_xmin16436_ymin1748_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-EW-A1P8-DX1_xmin15520_ymin26381_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0AV-DX1_xmin77937_ymin57165_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-D8-A143-DX1_xmin52429_ymin19478_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-OL-A97C-DX1_xmin68058_ymin32495_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-AR-A0U1-DX1_xmin63681_ymin24922_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A5ZV-DX1_xmin103009_ymin26581_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-BH-A0B9-DX1_xmin62727_ymin56697_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A14N-DX1_xmin21383_ymin66838_MPP-0.2500.png  \n",
            "  inflating: /content/__MACOSX/data/bcss/masks/._TCGA-E2-A14N-DX1_xmin21383_ymin66838_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A7-A26I-DX1_xmin9689_ymin25668_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-E2-A574-DX1_xmin54962_ymin47475_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XS-DX1_xmin21421_ymin37486_MPP-0.2500.png  \n",
            "  inflating: /content/data/bcss/masks/TCGA-A2-A3XU-DX1_xmin33329_ymin26157_MPP-0.2500.png  \n"
          ]
        }
      ],
      "source": [
        "# Replace 'My Drive/Path/to/your_data.zip' with the actual path\n",
        "ZIP_PATH = \"/content/drive/MyDrive/VFM Project/Copy of data.zip\"\n",
        "\n",
        "# The unzipped contents will go here (this is fast local storage)\n",
        "EXTRACT_PATH = \"/content/\"\n",
        "\n",
        "! unzip \"$ZIP_PATH\" -d \"$EXTRACT_PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXmW5roQDMiD",
        "outputId": "6f6e177a-0775-433d-c3ae-3bc04deb21c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFDQNpC2DMiD"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/microsoft/resnet-50\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/microsoft/resnet-50)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411,
          "referenced_widgets": [
            "fab54bade933463e8ad2dcc59d483b0e",
            "439f3fb88b0d4a6a82ee18546db26572",
            "98bc3131d39c4fe88e9751f24a4fcaf8",
            "5362e1ae3f254f3fa26a8bd628542bc7",
            "11b6a08298f54ed8a9130f5f68dc4f14",
            "325b645c3f4740d1bf7024a90433275c",
            "17bd7f26baad45099999000f256d73b2",
            "0e7515c09b5a42f2bd5a53541fb7a543",
            "b3e6f0f56b2f47d7b6f9e6f4a6cb63bb",
            "a98f6306be9b45299a2381b8bcbdeeca",
            "2c494ab61c694fe18f1d954ff297f108",
            "5a6512a9f3a24b2db7b494a797a1a2e7",
            "d3867416319449d5916f6fcefdf94bc9",
            "4ecce66839a247dbb04ca721fa6eb3f3",
            "3b36e7b294e24474a0c57fcf4fc4a296",
            "89bb286be437476a9a2e26efd800d350",
            "b31aa541ad964d428d370f14ce79646b",
            "6da4fc6fef7a418aab29d9d02f01a211",
            "3931082d6f734a0ea7c8163a611dba42",
            "5f201391685543cdb13ee9b493713492",
            "58be30e191f14cba9f7f71647e5c295f",
            "45bfa2806dc84abd9e8a9737c1fc0166",
            "b99b1af808364121a309c1133106099e",
            "47defcfb64814b06839bb04ea9fa2234",
            "9d92c4d7531641ab87608930b74b536a",
            "cee9ba8b738f447db6d6e0e50ca3aeac",
            "685f87cc03214134aef7360e73f0e91d",
            "ac33469592344b3ea2e618bdb0665d1b",
            "9e037046c02245bcb7e0c2fa8cfee78e",
            "8b6eeeb1852a40f5bd146bd5448f4772",
            "7f13e4f7eacc4a76ac9489f60faf69fd",
            "f375efdc2a5c407f9ef22bb573a74e3f",
            "21a4b6142c4d4188bedb6b6913bcd0e5"
          ]
        },
        "id": "uFKWF7ySDMiE",
        "outputId": "29c2717b-f18d-442f-c27f-37d4c32e757e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fab54bade933463e8ad2dcc59d483b0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a6512a9f3a24b2db7b494a797a1a2e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b99b1af808364121a309c1133106099e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'macaw', 'score': 0.9999986886978149},\n",
              " {'label': 'vulture', 'score': 2.1576444453330623e-07},\n",
              " {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
              "  'score': 2.3804846094321874e-08},\n",
              " {'label': 'cocktail shaker', 'score': 1.8397171785977662e-08},\n",
              " {'label': 'toucan', 'score': 1.1318117110192816e-08}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-classification\", model=\"microsoft/resnet-50\")\n",
        "pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SuGgJ6f1DMiE"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO7Xsq2fDMiE"
      },
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
        "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I2XCE2hTDMiF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r5_sTVceDMiF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"auto\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "# Using a public image URL instead of a local file that doesn't exist\n",
        "output = client.image_classification(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\", model=\"microsoft/resnet-50\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53205ad9"
      },
      "source": [
        "# Task\n",
        "Implement and train a Mask R-CNN model for instance segmentation on the BCSS dataset. This involves adapting the `BCSSDataset` to output instance masks and bounding boxes, configuring a pre-trained Mask R-CNN model from `torchvision`, and training the model. Finally, visualize sample predictions and provide a summary of the implementation and results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a03309d9"
      },
      "source": [
        "## Inspect and Adapt BCSSDataset for Mask R-CNN\n",
        "\n",
        "### Subtask:\n",
        "Inspect the existing `BCSSDataset` class and modify its `__getitem__` method to extract individual instance masks and bounding boxes for each object within an image, leveraging connected components analysis for each class in the provided mask.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "847b3eb4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `BCSSDataset` class to extract instance masks and bounding boxes. This involves using connected components analysis, which is provided by `cv2.connectedComponentsWithStats` since `cv2` is already imported. I will modify the `__getitem__` method to perform this extraction and format the output for Mask R-CNN training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82d013a"
      },
      "source": [
        "## Install PyTorch and Torchvision\n",
        "\n",
        "### Subtask:\n",
        "Install PyTorch and Torchvision, which provides pre-trained Mask R-CNN models and necessary utilities for object detection. Ensure the correct CUDA-enabled versions are installed if a GPU is available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbb37b1a"
      },
      "source": [
        "**Reasoning**:\n",
        "To install PyTorch and Torchvision, I will use `pip` commands. I will check for CUDA availability and provide the appropriate installation commands for PyTorch, followed by Torchvision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1807f667",
        "outputId": "729661ac-9b4b-44c9-d445-0062a180ee01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available! Installing PyTorch with CUDA support.\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "PyTorch and Torchvision installation complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available! Installing PyTorch with CUDA support.\")\n",
        "    # Command for CUDA 12.1 (adjust as needed for your CUDA version)\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "else:\n",
        "    print(\"CUDA not available. Installing PyTorch CPU version.\")\n",
        "    # Command for CPU only\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "print(\"PyTorch and Torchvision installation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44694378"
      },
      "source": [
        "## Refine BCSSDataset for Instance Segmentation Data\n",
        "\n",
        "### Subtask:\n",
        "Modify the `BCSSDataset` class to output instance segmentation data (bounding boxes, masks, labels) and implement a transformation pipeline for resizing, padding, and augmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28597836"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the transformation pipeline for instance segmentation, including `DetectionCompose`, `ResizeAndPadForDetection`, and `RandomHorizontalFlipForDetection` classes. Then, I will update the `BCSSDataset` class by modifying its `__init__` and `__getitem__` methods to incorporate these transformations and extract instance-level data (bounding boxes, masks, and labels) using connected components analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "183bcb89"
      },
      "source": [
        "## Load and Modify ResNet50 Model\n",
        "\n",
        "### Subtask:\n",
        "Load a pre-trained ResNet50 model from `torchvision.models` and replace its final classification layer for the BCSS dataset (5 classes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf4a39c"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the pre-trained ResNet50 model and modify its final classification layer for 5 classes, I will use `torchvision.models` to get the ResNet50 model with default weights, identify the input features of the final layer, replace it with a new `nn.Linear` layer with 5 output features, and then move the model to the appropriate device.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88710b86",
        "outputId": "495a6ad0-3fc5-45e5-ee53-47b70edabf73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 124MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50 model loaded and modified for 5 classes. Model moved to cuda.\n",
            "New final layer: Linear(in_features=2048, out_features=5, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# 1. Load a pre-trained ResNet50 model\n",
        "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# 2. Access the model's final fully connected layer (fc)\n",
        "# 3. Determine the number of input features to this final layer\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "# 4. Create a new torch.nn.Linear layer with 5 output features\n",
        "# (5 classes: tumor, stroma, lymphocyte, necrosis, blood_vessel)\n",
        "num_classes = 5 # Excluding background, which is class 0 and not an instance\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# 5. Move the modified model to the appropriate device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"ResNet50 model loaded and modified for {num_classes} classes. Model moved to {device}.\")\n",
        "print(f\"New final layer: {model.fc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63537d51"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block executed successfully, indicating that the ResNet50 model was loaded and its final layer was modified as intended. The next logical step is to define the optimizer and the loss function for training this classification model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5e1d407",
        "outputId": "8b7a91a4-2012-4685-bbb0-48389aed1eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer and loss function defined.\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss for classification)\n",
        "# Assuming class labels are 1-indexed (1 to 5) as per the dataset description,\n",
        "# or adjust if they are 0-indexed.\n",
        "# If labels are 1-indexed, they might need to be shifted by -1 for CrossEntropyLoss\n",
        "# which expects class indices from 0 to num_classes - 1.\n",
        "# Let's confirm the labels generated by the BCSSDataset first. Currently, num_classes is 5.\n",
        "# The instance labels in BCSSDataset are 1: 'tumor', 2: 'stroma', 3: 'lymphocyte', 4: 'necrosis', 5: 'blood_vessel'\n",
        "# So, CrossEntropyLoss expects target labels to be in range [0, 4]. We need to subtract 1 from the labels.\n",
        "class_labels_start_idx = 1 # The smallest class_id is 1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Optimizer and loss function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26ad560b",
        "outputId": "da8eae01-f3a3-4c5c-f3b8-97e1f46478a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders re-initialized with custom collate_fn, transformations, and num_workers=0.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms.functional as F_vision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Helper for transformations\n",
        "class DetectionCompose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target, mask):\n",
        "        for t in self.transforms:\n",
        "            image, target, mask = t(image, target, mask)\n",
        "        return image, target, mask\n",
        "\n",
        "class ResizeAndPadForDetection:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, image, target, mask):\n",
        "        # Convert PIL Image to numpy array for easier manipulation\n",
        "        image_np = np.array(image)\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "        h, w = image_np.shape[:2]\n",
        "        scale = self.size / max(h, w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # Resize image and mask\n",
        "        resized_image = cv2.resize(image_np, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "        resized_mask = cv2.resize(mask_np, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Create a new padded image and mask\n",
        "        padded_image = np.zeros((self.size, self.size, 3), dtype=resized_image.dtype)\n",
        "        padded_mask = np.zeros((self.size, self.size), dtype=resized_mask.dtype)\n",
        "\n",
        "        # Place the resized image and mask in the center\n",
        "        start_h = (self.size - new_h) // 2\n",
        "        start_w = (self.size - new_w) // 2\n",
        "        padded_image[start_h : start_h + new_h, start_w : start_w + new_w] = resized_image\n",
        "        padded_mask[start_h : start_h + new_h, start_w : start_w + new_w] = resized_mask\n",
        "\n",
        "        # Adjust bounding boxes\n",
        "        boxes = target['boxes']\n",
        "        if boxes.numel() > 0:\n",
        "            boxes_np = boxes.cpu().numpy()\n",
        "            boxes_np[:, [0, 2]] = boxes_np[:, [0, 2]] * scale + start_w  # xmin, xmax\n",
        "            boxes_np[:, [1, 3]] = boxes_np[:, [1, 3]] * scale + start_h  # ymin, ymax\n",
        "            target['boxes'] = torch.from_numpy(boxes_np).float()\n",
        "\n",
        "        # Adjust masks (instance masks)\n",
        "        instance_masks = target['masks']\n",
        "        if instance_masks.numel() > 0:\n",
        "            # Resize each instance mask\n",
        "            resized_instance_masks = []\n",
        "            for i in range(instance_masks.shape[0]):\n",
        "                mask_i_np = instance_masks[i].cpu().numpy()\n",
        "                resized_mask_i = cv2.resize(mask_i_np.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "                padded_mask_i = np.zeros((self.size, self.size), dtype=np.uint8)\n",
        "                padded_mask_i[start_h : start_h + new_h, start_w : start_w + new_w] = resized_mask_i\n",
        "                resized_instance_masks.append(torch.from_numpy(padded_mask_i))\n",
        "            target['masks'] = torch.stack(resized_instance_masks)\n",
        "\n",
        "        image = F_vision.to_tensor(padded_image.astype(np.float32) / 255.0) # Normalize to [0,1]\n",
        "\n",
        "        return image, target, torch.from_numpy(padded_mask).long()\n",
        "\n",
        "class RandomHorizontalFlipForDetection:\n",
        "    def __init__(self, prob=0.5):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target, mask):\n",
        "        if torch.rand(1) < self.prob:\n",
        "            image = F_vision.hflip(image)\n",
        "            mask = F_vision.hflip(mask.unsqueeze(0)).squeeze(0) # Apply to semantic mask\n",
        "\n",
        "            # Flip bounding boxes\n",
        "            boxes = target['boxes']\n",
        "            if boxes.numel() > 0:\n",
        "                image_width = image.shape[2] # Image is CxHxW\n",
        "                xmin = boxes[:, 0]\n",
        "                xmax = boxes[:, 2]\n",
        "                new_xmin = image_width - xmax\n",
        "                new_xmax = image_width - xmin\n",
        "                boxes[:, 0] = new_xmin\n",
        "                boxes[:, 2] = new_xmax\n",
        "                target['boxes'] = boxes\n",
        "\n",
        "            # Flip instance masks\n",
        "            instance_masks = target['masks']\n",
        "            if instance_masks.numel() > 0:\n",
        "                target['masks'] = F_vision.hflip(instance_masks) # Masks are NxHxW\n",
        "\n",
        "        return image, target, mask\n",
        "\n",
        "def get_transform(train, img_size):\n",
        "    transforms = []\n",
        "    transforms.append(ResizeAndPadForDetection(img_size))\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlipForDetection(0.5))\n",
        "    return DetectionCompose(transforms)\n",
        "\n",
        "\n",
        "class BCSSDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, split='train'):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        all_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
        "\n",
        "        test_prefixes = ['TCGA-OL-', 'TCGA-LL-', 'TCGA-E2-', 'TCGA-EW-', 'TCGA-GM-', 'TCGA-S3-']\n",
        "\n",
        "        test_files = [f for f in all_files if any(f.startswith(p) for p in test_prefixes)]\n",
        "        train_val_files = [f for f in all_files if f not in test_files]\n",
        "\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(train_val_files)\n",
        "        val_split = int(0.2 * len(train_val_files))\n",
        "        val_files = train_val_files[:val_split]\n",
        "        train_files = train_val_files[val_split:]\n",
        "\n",
        "        if split == 'train':\n",
        "            initial_image_files = train_files\n",
        "        elif split == 'val':\n",
        "            initial_image_files = val_files\n",
        "        elif split == 'test':\n",
        "            initial_image_files = test_files\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split: {split}. Must be 'train', 'val', or 'test'.\")\n",
        "\n",
        "        self.class_names = {\n",
        "            0: 'background',\n",
        "            1: 'tumor',\n",
        "            2: 'stroma',\n",
        "            3: 'lymphocyte',\n",
        "            4: 'necrosis',\n",
        "            5: 'blood_vessel'\n",
        "        }\n",
        "\n",
        "        # Filter out images that have no instances (only background in mask)\n",
        "        self.image_files = []\n",
        "        for filename in initial_image_files:\n",
        "            mask_path = os.path.join(self.mask_dir, filename)\n",
        "            mask = np.array(Image.open(mask_path))\n",
        "            unique_classes_in_mask = np.unique(mask)\n",
        "            unique_classes_in_mask = unique_classes_in_mask[unique_classes_in_mask != 0] # Exclude background\n",
        "            if len(unique_classes_in_mask) > 0:\n",
        "                self.image_files.append(filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        mask_path = os.path.join(self.mask_dir, self.image_files[idx])\n",
        "        mask = np.array(Image.open(mask_path)) # Semantic mask\n",
        "\n",
        "        # Store original numpy array of image before transformations\n",
        "        image_np_original = np.array(image)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        instance_masks = []\n",
        "\n",
        "        unique_classes_in_mask = np.unique(mask)\n",
        "        unique_classes_in_mask = unique_classes_in_mask[unique_classes_in_mask != 0] # Exclude background\n",
        "\n",
        "        # Since we filtered out images with no instances in __init__,\n",
        "        # unique_classes_in_mask will always have length > 0 here.\n",
        "\n",
        "        for class_id in unique_classes_in_mask:\n",
        "            # Create a binary mask for the current class\n",
        "            class_binary_mask = (mask == class_id).astype(np.uint8)\n",
        "\n",
        "            # Find connected components for instance segmentation\n",
        "            num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(class_binary_mask, 8, cv2.CV_32S)\n",
        "\n",
        "            for i in range(1, num_labels): # Iterate through instances, skipping background (label 0)\n",
        "                xmin = stats[i, cv2.CC_STAT_LEFT]\n",
        "                ymin = stats[i, cv2.CC_STAT_TOP]\n",
        "                width = stats[i, cv2.CC_STAT_WIDTH]\n",
        "                height = stats[i, cv2.CC_STAT_HEIGHT]\n",
        "                xmax = xmin + width\n",
        "                ymax = ymin + height\n",
        "\n",
        "                # Ensure valid bounding box\n",
        "                if width > 0 and height > 0:\n",
        "                    boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    labels.append(class_id) # Assign the semantic class_id as instance label\n",
        "\n",
        "                    # Create instance mask for the current component\n",
        "                    instance_mask = (labels_img == i).astype(np.uint8)\n",
        "                    instance_masks.append(instance_mask)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        # After filtering in __init__, boxes should ideally never be empty here.\n",
        "        # Keeping the check as a safeguard, although it should rarely be triggered.\n",
        "        if len(boxes) == 0:\n",
        "            print(f\"Warning: Image {self.image_files[idx]} still produced no instances after filtering in __init__.\")\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            instance_masks = torch.zeros((0, mask.shape[0], mask.shape[1]), dtype=torch.uint8)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "            instance_masks = torch.as_tensor(np.array(instance_masks), dtype=torch.uint8)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'masks': instance_masks\n",
        "        }\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image_pil = image # Keep image as PIL for initial transform input\n",
        "            image, target, mask = self.transform(image_pil, target, mask)\n",
        "        else:\n",
        "            image = F_vision.to_tensor(image_np_original.astype(np.float32) / 255.0)\n",
        "            mask = torch.from_numpy(mask).long() # Ensure mask is long for semantic segmentation\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'target': target, # Now contains boxes, labels, and instance_masks\n",
        "            'unique_classes': torch.as_tensor(unique_classes_in_mask, dtype=torch.int64),\n",
        "            'filename': self.image_files[idx],\n",
        "            'image_np': image_np_original # Keep original image_np for debugging/visualization purposes if needed\n",
        "        }\n",
        "\n",
        "def instance_collate_fn(batch):\n",
        "    all_cropped_images = []\n",
        "    all_instance_labels = []\n",
        "\n",
        "    # Define a maximum number of instances to process per batch (which is per image since DataLoader batch_size is 1)\n",
        "    max_instances_per_batch = 64 # Conservative limit to ensure memory fit\n",
        "\n",
        "    for item in batch:\n",
        "        image = item['image'] # C, H, W tensor, already normalized\n",
        "        boxes = item['target']['boxes'] # N, 4 tensor of xmin, ymin, xmax, ymax\n",
        "        labels = item['target']['labels'] # N tensor of class labels\n",
        "\n",
        "        # If there are more instances than allowed, sample a subset\n",
        "        if boxes.shape[0] > max_instances_per_batch:\n",
        "            indices = torch.randperm(boxes.shape[0])[:max_instances_per_batch]\n",
        "            boxes = boxes[indices]\n",
        "            labels = labels[indices]\n",
        "\n",
        "        # Iterate through each instance in the current image (or sampled subset)\n",
        "        for i in range(boxes.shape[0]):\n",
        "            box = boxes[i]\n",
        "            label = labels[i]\n",
        "\n",
        "            # Extract coordinates for cropping (xmin, ymin, xmax, ymax)\n",
        "            xmin, ymin, xmax, ymax = box.int().tolist()\n",
        "\n",
        "            # Calculate height and width for F_vision.crop\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "\n",
        "            # Ensure valid crop dimensions\n",
        "            if width > 0 and height > 0:\n",
        "                # Crop the instance from the image\n",
        "                cropped_image = F_vision.crop(image, ymin, xmin, height, width)\n",
        "\n",
        "                all_cropped_images.append(cropped_image)\n",
        "                all_instance_labels.append(label)\n",
        "\n",
        "    # If no instances were found in the entire batch, return empty tensors\n",
        "    # This case should ideally not be reached if BCSSDataset filters correctly.\n",
        "    if not all_cropped_images:\n",
        "        return torch.empty((0, 3, 32, 32)), torch.empty((0,), dtype=torch.int64)\n",
        "\n",
        "    # For simplicity, we'll resize each cropped image to a fixed size (e.g., 32x32)\n",
        "    # to make them stackable into a single batch tensor for a potential classifier.\n",
        "    target_instance_size = (32, 32) # Further reducing target instance size\n",
        "    processed_cropped_images = []\n",
        "    for img_tensor in all_cropped_images:\n",
        "        # Ensure image is 3 channels, resize using bicubic interpolation\n",
        "        if img_tensor.shape[0] == 1: # If grayscale, convert to 3 channels by repeating\n",
        "            img_tensor = img_tensor.repeat(3, 1, 1)\n",
        "        elif img_tensor.shape[0] == 4: # If RGBA, convert to RGB\n",
        "            img_tensor = img_tensor[:3, :, :]\n",
        "\n",
        "        # Resize to target_instance_size\n",
        "        resized_img = F_vision.resize(img_tensor, target_instance_size, interpolation=F_vision.InterpolationMode.BICUBIC)\n",
        "        processed_cropped_images.append(resized_img)\n",
        "\n",
        "    stacked_cropped_images = torch.stack(processed_cropped_images)\n",
        "    stacked_instance_labels = torch.stack(all_instance_labels)\n",
        "\n",
        "    return stacked_cropped_images, stacked_instance_labels\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "images_path = '/content/data/bcss/images'\n",
        "masks_path = '/content/data/bcss/masks'\n",
        "\n",
        "train_dataset = BCSSDataset(images_path, masks_path, transform=get_transform(train=True, img_size=IMG_SIZE), split='train')\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=instance_collate_fn)\n",
        "\n",
        "val_dataset = BCSSDataset(images_path, masks_path, transform=get_transform(train=False, img_size=IMG_SIZE), split='val')\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=0, collate_fn=instance_collate_fn)\n",
        "\n",
        "test_dataset = BCSSDataset(images_path, masks_path, transform=get_transform(train=False, img_size=IMG_SIZE), split='test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=0, collate_fn=instance_collate_fn)\n",
        "\n",
        "print(\"DataLoaders re-initialized with custom collate_fn, transformations, and num_workers=0.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db6dbdb"
      },
      "source": [
        "**Reasoning**:\n",
        "The DataLoaders have been re-initialized with `num_workers=0` to address the previous `RuntimeError`. Now, the next step is to re-execute the `train_model` function to proceed with training and determine if the worker error is resolved or if other issues arise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "57c71786",
        "outputId": "f31d7ef4-ce26-4c8a-95b2-ee4f57ca5f00"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1977997876.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Call the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels_start_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# Set environment variable to avoid CUDA memory fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "# Set CUDA_LAUNCH_BLOCKING to 1 for more detailed error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, class_labels_start_idx):\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            # Skip empty batches if instance_collate_fn returns them\n",
        "            if inputs.numel() == 0:\n",
        "                print(f\"  Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}]: Skipping empty batch.\")\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            # Adjust labels for CrossEntropyLoss (0-indexed)\n",
        "            labels = (labels - class_labels_start_idx).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0: # Print every 10 batches\n",
        "                print(f\"  Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_duration = end_time - start_time\n",
        "        print(f\"Epoch {epoch+1} finished. Avg Loss: {running_loss/len(train_loader):.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # Skip empty batches during validation\n",
        "                if inputs.numel() == 0:\n",
        "                    print(f\"  Validation: Skipping empty batch.\")\n",
        "                    continue\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                labels = (labels - class_labels_start_idx).to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "# Call the training function\n",
        "num_epochs = 10\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, class_labels_start_idx)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fab54bade933463e8ad2dcc59d483b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_439f3fb88b0d4a6a82ee18546db26572",
              "IPY_MODEL_98bc3131d39c4fe88e9751f24a4fcaf8",
              "IPY_MODEL_5362e1ae3f254f3fa26a8bd628542bc7"
            ],
            "layout": "IPY_MODEL_11b6a08298f54ed8a9130f5f68dc4f14"
          }
        },
        "439f3fb88b0d4a6a82ee18546db26572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_325b645c3f4740d1bf7024a90433275c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_17bd7f26baad45099999000f256d73b2",
            "value": "config.json:‚Äá"
          }
        },
        "98bc3131d39c4fe88e9751f24a4fcaf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e7515c09b5a42f2bd5a53541fb7a543",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3e6f0f56b2f47d7b6f9e6f4a6cb63bb",
            "value": 1
          }
        },
        "5362e1ae3f254f3fa26a8bd628542bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a98f6306be9b45299a2381b8bcbdeeca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2c494ab61c694fe18f1d954ff297f108",
            "value": "‚Äá69.6k/?‚Äá[00:00&lt;00:00,‚Äá5.94MB/s]"
          }
        },
        "11b6a08298f54ed8a9130f5f68dc4f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "325b645c3f4740d1bf7024a90433275c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17bd7f26baad45099999000f256d73b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7515c09b5a42f2bd5a53541fb7a543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b3e6f0f56b2f47d7b6f9e6f4a6cb63bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a98f6306be9b45299a2381b8bcbdeeca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c494ab61c694fe18f1d954ff297f108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a6512a9f3a24b2db7b494a797a1a2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3867416319449d5916f6fcefdf94bc9",
              "IPY_MODEL_4ecce66839a247dbb04ca721fa6eb3f3",
              "IPY_MODEL_3b36e7b294e24474a0c57fcf4fc4a296"
            ],
            "layout": "IPY_MODEL_89bb286be437476a9a2e26efd800d350"
          }
        },
        "d3867416319449d5916f6fcefdf94bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b31aa541ad964d428d370f14ce79646b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6da4fc6fef7a418aab29d9d02f01a211",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "4ecce66839a247dbb04ca721fa6eb3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3931082d6f734a0ea7c8163a611dba42",
            "max": 102482854,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f201391685543cdb13ee9b493713492",
            "value": 102482854
          }
        },
        "3b36e7b294e24474a0c57fcf4fc4a296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58be30e191f14cba9f7f71647e5c295f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_45bfa2806dc84abd9e8a9737c1fc0166",
            "value": "‚Äá102M/102M‚Äá[00:02&lt;00:00,‚Äá58.6MB/s]"
          }
        },
        "89bb286be437476a9a2e26efd800d350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b31aa541ad964d428d370f14ce79646b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da4fc6fef7a418aab29d9d02f01a211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3931082d6f734a0ea7c8163a611dba42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f201391685543cdb13ee9b493713492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58be30e191f14cba9f7f71647e5c295f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45bfa2806dc84abd9e8a9737c1fc0166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b99b1af808364121a309c1133106099e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47defcfb64814b06839bb04ea9fa2234",
              "IPY_MODEL_9d92c4d7531641ab87608930b74b536a",
              "IPY_MODEL_cee9ba8b738f447db6d6e0e50ca3aeac"
            ],
            "layout": "IPY_MODEL_685f87cc03214134aef7360e73f0e91d"
          }
        },
        "47defcfb64814b06839bb04ea9fa2234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac33469592344b3ea2e618bdb0665d1b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9e037046c02245bcb7e0c2fa8cfee78e",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "9d92c4d7531641ab87608930b74b536a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b6eeeb1852a40f5bd146bd5448f4772",
            "max": 266,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f13e4f7eacc4a76ac9489f60faf69fd",
            "value": 266
          }
        },
        "cee9ba8b738f447db6d6e0e50ca3aeac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f375efdc2a5c407f9ef22bb573a74e3f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_21a4b6142c4d4188bedb6b6913bcd0e5",
            "value": "‚Äá266/266‚Äá[00:00&lt;00:00,‚Äá7.34kB/s]"
          }
        },
        "685f87cc03214134aef7360e73f0e91d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac33469592344b3ea2e618bdb0665d1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e037046c02245bcb7e0c2fa8cfee78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b6eeeb1852a40f5bd146bd5448f4772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f13e4f7eacc4a76ac9489f60faf69fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f375efdc2a5c407f9ef22bb573a74e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21a4b6142c4d4188bedb6b6913bcd0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}