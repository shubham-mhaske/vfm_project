# 6-Minute Academic Presentation Guide

## ğŸ“Š PUBLICATION-QUALITY FIGURES

### Qualitative Results (Real Test Images) â­ NEW
Location: `results/figures/qualitative/`

| Figure | Description | Recommended Slide |
|--------|-------------|-------------------|
| **qualitative_method_comparison.png** | 4 test images Ã— 6 methods side-by-side | Slide 5-6: Main Results â­â­ |
| **qualitative_per_class.png** | Per-tissue class segmentation examples | Slide 7: Per-Class Analysis |
| **qualitative_prompt_comparison.png** | Prompt strategy visual comparison | Slide 4: Prompt Ablation |
| **qualitative_success_failure.png** | Success (Dice>0.7) vs Failure (Dice<0.4) | Slide 10: Limitations |
| **qualitative_full_segmentation.png** | Full multi-class colored segmentation | Slide 3: Pipeline Demo |

### Academic Charts (Metrics-Based)
Location: `results/figures/academic/`

| Figure | Description | Recommended Slide |
|--------|-------------|-------------------|
| **fig4_method_overview.png** | Pipeline architecture schematic | Slide 3: Methods |
| **fig1_segmentation_comprehensive.png** | 4-panel segmentation analysis | Slide 8: Quantitative Results |
| **fig2_clip_analysis.png** | 3-panel CLIP classification | Slide 9: CLIP Results |
| **fig3_training_analysis.png** | 3-panel training curves | Slide 10: Why Finetuning Failed |
| **fig5_summary_results.png** | Complete results tables | Slide 11: Summary |

---

## â±ï¸ 12-SLIDE STRUCTURE (6 minutes)

| Slide | Time | Figure | Key Message |
|-------|------|--------|-------------|
| 1. Title | 10s | Sample image | VFMs for Medical Image Segmentation |
| 2. Problem | 30s | - | Manual annotation costly; VFMs pretrained on billions |
| 3. Methods | 40s | `academic/fig4_method_overview.png` | Two-stage: SAM2â†’CLIP pipeline |
| 4. Dataset | 20s | - | BCSS: 151 images, 5 classes, 45 test |
| **5-6. Segmentation** | **60s** | **`qualitative/qualitative_method_comparison.png`** | **Real predictions! Box+Neg best (0.555)** |
| **7. Per-Class** | **30s** | **`qualitative/qualitative_per_class.png`** | **Visual per-class performance** |
| **8. CLIP** | **40s** | **`academic/fig2_clip_analysis.png`** | **Few-shot LLM best (44.4%)** |
| **9. Training** | **40s** | **`academic/fig3_training_analysis.png`** | **Overfitting evidence** |
| 10. Limitations | 20s | `qualitative/qualitative_success_failure.png` | Success vs failure cases |
| 11. Summary | 30s | `academic/fig5_summary_results.png` | All results in tables |
| 12. Thank You | 10s | - | Questions |

---

## ğŸ”‘ KEY NUMBERS TO MEMORIZE

| Metric | Value | Context |
|--------|-------|---------|
| **0.555** | Dice | SAM2 zero-shot (Box+Neg) - BEST |
| **-33%** | Drop | Finetuning vs zero-shot |
| **+64%** | Gain | Box vs point prompts |
| **44.4%** | Accuracy | CLIP with LLM few-shot prompts |
| **+264%** | Gain | Prompt evolution v1â†’v3 |

---

## ğŸ“‹ WHAT EACH FIGURE SHOWS

### Fig 1: Segmentation Comprehensive (4 panels)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) Prompt Ablation        â”‚ (b) Model Comparison                   â”‚
â”‚ â€¢ Centroid: 0.338          â”‚ â€¢ SAM2 Box+Neg: 0.555 (best)          â”‚
â”‚ â€¢ Multi-Pt: 0.418          â”‚ â€¢ SAM2 Box: 0.553                     â”‚
â”‚ â€¢ Box: 0.553               â”‚ â€¢ MedSAM+TTA: 0.536                   â”‚
â”‚ â€¢ Box+Neg: 0.555 (+64%)    â”‚ â€¢ MedSAM Box: 0.522                   â”‚
â”‚                            â”‚ â€¢ p < 0.05 significance               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ (c) Zero-Shot vs Finetuned â”‚ (d) Per-Class Heatmap                 â”‚
â”‚ â€¢ Zero-shot: 0.555         â”‚ â€¢ Necrosis: 0.69 (easiest)            â”‚
â”‚ â€¢ Focal 50ep: 0.372 (-33%) â”‚ â€¢ Tumor: 0.56                         â”‚
â”‚ â€¢ BCE 100ep: 0.371 (-33%)  â”‚ â€¢ Stroma: 0.54                        â”‚
â”‚ â€¢ LoRA 30ep: 0.355 (-36%)  â”‚ â€¢ Blood Vessel: 0.50 (hardest)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fig 2: CLIP Analysis (3 panels)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) Strategy Comparison    â”‚ (b) Per-Class    â”‚ (c) Evolution       â”‚
â”‚ â€¢ LLM Few-Shot: 44.4%     â”‚ Per-class acc    â”‚ v1 Jargon: 12.2%   â”‚
â”‚ â€¢ Manual v2: 42.2%        â”‚ for 3 methods    â”‚ v2 Optimized: 35.6%â”‚
â”‚ â€¢ LLM Text v2: 35.6%      â”‚ across 5 classes â”‚ v3 Few-Shot: 44.4% â”‚
â”‚ â€¢ LLM VLM: 29.4%          â”‚                  â”‚ (+264% gain)        â”‚
â”‚ â€¢ Manual v1: 23.3%        â”‚                  â”‚                     â”‚
â”‚ â€¢ LLM VLM v1: 8.3%        â”‚                  â”‚ Text > Multimodal   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fig 3: Training Analysis (3 panels)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) Training Loss          â”‚ (b) Validation Dice  â”‚ (c) Final Test â”‚
â”‚ â€¢ Rapid convergence       â”‚ â€¢ Peak at epoch 8    â”‚ â€¢ Zero-shot bestâ”‚
â”‚ â€¢ All methods â†’ 0.1       â”‚ â€¢ Then declines      â”‚ â€¢ All finetuned â”‚
â”‚ â€¢ Classic overfitting     â”‚ â€¢ Overfitting region â”‚   worse by 33%+ â”‚
â”‚   pattern                 â”‚   shaded             â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fig 4: Method Overview
- Complete pipeline schematic
- Stage 1: Input â†’ SAM2/MedSAM (with prompts) â†’ Binary Mask
- Stage 2: Crop â†’ CLIP (with text prompts) â†’ Class Label
- Model parameters and dataset info annotated

### Fig 5: Summary Tables
- Table (a): All segmentation configurations with Dice/IoU
- Table (b): Finetuning methods comparison
- Table (c): All CLIP prompt strategies

---

## ğŸ—£ï¸ 60-SECOND SPEAKING SCRIPTS

### Slide 5-6 (Segmentation - 60s):
"This figure shows our complete segmentation analysis. Panel A demonstrates that box prompts dramatically outperform point prompts - a 64% improvement in Dice score. Panel B compares SAM2 and MedSAM, with SAM2 achieving the best performance at 0.555 Dice. Critically, Panel C shows that all finetuning attempts hurt performance by 33% compared to zero-shot. Panel D breaks down per-class performance, showing necrosis is easiest and blood vessels most challenging."

### Slide 7 (CLIP - 40s):
"For CLIP classification, we tested 8 prompt strategies. The key finding is that LLM-generated prompts with few-shot examples achieved 44.4% accuracy - the best result. Panel C shows the evolution from medical jargon at 12% to optimized visual language at 44% - a 264% improvement. Importantly, text-only LLM beat multimodal, because images triggered medical vocabulary CLIP doesn't understand."

### Slide 8 (Training - 40s):
"Why did finetuning fail? Panel A shows training loss converging rapidly - a sign of overfitting to our small 85-image dataset. Panel B reveals validation Dice peaks at epoch 8 then declines as the model forgets pretrained features. Panel C confirms: zero-shot outperforms all finetuned variants. This is catastrophic forgetting on a small medical dataset."

---

## âœ… PRESENTATION CHECKLIST

- [ ] Load `fig4_method_overview.png` for methods slide
- [ ] Load `fig1_segmentation_comprehensive.png` for main results
- [ ] Load `fig2_clip_analysis.png` for CLIP results  
- [ ] Load `fig3_training_analysis.png` for training analysis
- [ ] Load `fig5_summary_results.png` for summary
- [ ] Memorize: 0.555, -33%, +64%, 44.4%
- [ ] Practice 6-minute timing (aim for 5:30)

---

## ğŸ“ FILE LOCATIONS

```
results/figures/
â”œâ”€â”€ qualitative/                              â† REAL TEST IMAGES â­
â”‚   â”œâ”€â”€ qualitative_method_comparison.png     â† MAIN PRESENTATION FIGURE
â”‚   â”œâ”€â”€ qualitative_per_class.png             â† Per-class examples
â”‚   â”œâ”€â”€ qualitative_prompt_comparison.png     â† Prompt ablation visual
â”‚   â”œâ”€â”€ qualitative_success_failure.png       â† Success vs failure
â”‚   â””â”€â”€ qualitative_full_segmentation.png     â† Multi-class demo
â”‚
â”œâ”€â”€ academic/                                 â† METRICS-BASED CHARTS
â”‚   â”œâ”€â”€ fig1_segmentation_comprehensive.png   â† 4-panel quantitative
â”‚   â”œâ”€â”€ fig2_clip_analysis.png                â† CLIP results
â”‚   â”œâ”€â”€ fig3_training_analysis.png            â† Training curves
â”‚   â”œâ”€â”€ fig4_method_overview.png              â† Pipeline schematic
â”‚   â””â”€â”€ fig5_summary_results.png              â† Summary tables
â”‚
â””â”€â”€ All figures also available as PDF for high-quality printing.
```

### Generation Scripts

```bash
# Generate qualitative figures (requires GPU + SAM2/MedSAM)
sbatch scripts/slurm/run_qualitative_figures.slurm

# Generate academic charts (CPU only, from metrics)
python scripts/analysis/generate_academic_figures.py
```
