# 6-Minute Academic Presentation Guide

## ğŸ“Š PUBLICATION-QUALITY FIGURES

Location: `results/figures/academic/`

| Figure | Description | Recommended Slide |
|--------|-------------|-------------------|
| **fig4_method_overview.png** | Pipeline architecture schematic | Slide 3: Methods |
| **fig1_segmentation_comprehensive.png** | 4-panel segmentation analysis | Slide 5-6: Main Results â­ |
| **fig2_clip_analysis.png** | 3-panel CLIP classification | Slide 7: CLIP Results â­ |
| **fig3_training_analysis.png** | 3-panel training curves | Slide 8: Why Finetuning Failed â­ |
| **fig5_summary_results.png** | Complete results tables | Slide 9: Summary |

---

## â±ï¸ 12-SLIDE STRUCTURE (6 minutes)

| Slide | Time | Figure | Key Message |
|-------|------|--------|-------------|
| 1. Title | 10s | Sample image | VFMs for Medical Image Segmentation |
| 2. Problem | 30s | - | Manual annotation costly; VFMs pretrained on billions |
| 3. Methods | 40s | `fig4_method_overview.png` | Two-stage: SAM2â†’CLIP pipeline |
| 4. Dataset | 20s | - | BCSS: 151 images, 5 classes, 45 test |
| **5-6. Segmentation** | **60s** | **`fig1_segmentation_comprehensive.png`** | **Box+Neg best (0.555), finetuning hurts (-33%)** |
| **7. CLIP** | **40s** | **`fig2_clip_analysis.png`** | **Few-shot LLM best (44.4%), text > multimodal** |
| **8. Training** | **40s** | **`fig3_training_analysis.png`** | **Overfitting evidence, catastrophic forgetting** |
| 9. Summary | 30s | `fig5_summary_results.png` | All results in tables |
| 10. Key Findings | 30s | - | 4 takeaways |
| 11. Conclusion | 20s | - | Best config: SAM2 Box+Neg + LLM Few-Shot |
| 12. Thank You | 10s | - | Questions |

---

## ğŸ”‘ KEY NUMBERS TO MEMORIZE

| Metric | Value | Context |
|--------|-------|---------|
| **0.555** | Dice | SAM2 zero-shot (Box+Neg) - BEST |
| **-33%** | Drop | Finetuning vs zero-shot |
| **+64%** | Gain | Box vs point prompts |
| **44.4%** | Accuracy | CLIP with LLM few-shot prompts |
| **+264%** | Gain | Prompt evolution v1â†’v3 |

---

## ğŸ“‹ WHAT EACH FIGURE SHOWS

### Fig 1: Segmentation Comprehensive (4 panels)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) Prompt Ablation        â”‚ (b) Model Comparison                   â”‚
â”‚ â€¢ Centroid: 0.338          â”‚ â€¢ SAM2 Box+Neg: 0.555 (best)          â”‚
â”‚ â€¢ Multi-Pt: 0.418          â”‚ â€¢ SAM2 Box: 0.553                     â”‚
â”‚ â€¢ Box: 0.553               â”‚ â€¢ MedSAM+TTA: 0.536                   â”‚
â”‚ â€¢ Box+Neg: 0.555 (+64%)    â”‚ â€¢ MedSAM Box: 0.522                   â”‚
â”‚                            â”‚ â€¢ p < 0.05 significance               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ (c) Zero-Shot vs Finetuned â”‚ (d) Per-Class Heatmap                 â”‚
â”‚ â€¢ Zero-shot: 0.555         â”‚ â€¢ Necrosis: 0.69 (easiest)            â”‚
â”‚ â€¢ Focal 50ep: 0.372 (-33%) â”‚ â€¢ Tumor: 0.56                         â”‚
â”‚ â€¢ BCE 100ep: 0.371 (-33%)  â”‚ â€¢ Stroma: 0.54                        â”‚
â”‚ â€¢ LoRA 30ep: 0.355 (-36%)  â”‚ â€¢ Blood Vessel: 0.50 (hardest)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fig 2: CLIP Analysis (3 panels)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) Strategy Comparison    â”‚ (b) Per-Class    â”‚ (c) Evolution       â”‚
â”‚ â€¢ LLM Few-Shot: 44.4%     â”‚ Per-class acc    â”‚ v1 Jargon: 12.2%   â”‚
â”‚ â€¢ Manual v2: 42.2%        â”‚ for 3 methods    â”‚ v2 Optimized: 35.6%â”‚
â”‚ â€¢ LLM Text v2: 35.6%      â”‚ across 5 classes â”‚ v3 Few-Shot: 44.4% â”‚
â”‚ â€¢ LLM VLM: 29.4%          â”‚                  â”‚ (+264% gain)        â”‚
â”‚ â€¢ Manual v1: 23.3%        â”‚                  â”‚                     â”‚
â”‚ â€¢ LLM VLM v1: 8.3%        â”‚                  â”‚ Text > Multimodal   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fig 3: Training Analysis (3 panels)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) Training Loss          â”‚ (b) Validation Dice  â”‚ (c) Final Test â”‚
â”‚ â€¢ Rapid convergence       â”‚ â€¢ Peak at epoch 8    â”‚ â€¢ Zero-shot bestâ”‚
â”‚ â€¢ All methods â†’ 0.1       â”‚ â€¢ Then declines      â”‚ â€¢ All finetuned â”‚
â”‚ â€¢ Classic overfitting     â”‚ â€¢ Overfitting region â”‚   worse by 33%+ â”‚
â”‚   pattern                 â”‚   shaded             â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fig 4: Method Overview
- Complete pipeline schematic
- Stage 1: Input â†’ SAM2/MedSAM (with prompts) â†’ Binary Mask
- Stage 2: Crop â†’ CLIP (with text prompts) â†’ Class Label
- Model parameters and dataset info annotated

### Fig 5: Summary Tables
- Table (a): All segmentation configurations with Dice/IoU
- Table (b): Finetuning methods comparison
- Table (c): All CLIP prompt strategies

---

## ğŸ—£ï¸ 60-SECOND SPEAKING SCRIPTS

### Slide 5-6 (Segmentation - 60s):
"This figure shows our complete segmentation analysis. Panel A demonstrates that box prompts dramatically outperform point prompts - a 64% improvement in Dice score. Panel B compares SAM2 and MedSAM, with SAM2 achieving the best performance at 0.555 Dice. Critically, Panel C shows that all finetuning attempts hurt performance by 33% compared to zero-shot. Panel D breaks down per-class performance, showing necrosis is easiest and blood vessels most challenging."

### Slide 7 (CLIP - 40s):
"For CLIP classification, we tested 8 prompt strategies. The key finding is that LLM-generated prompts with few-shot examples achieved 44.4% accuracy - the best result. Panel C shows the evolution from medical jargon at 12% to optimized visual language at 44% - a 264% improvement. Importantly, text-only LLM beat multimodal, because images triggered medical vocabulary CLIP doesn't understand."

### Slide 8 (Training - 40s):
"Why did finetuning fail? Panel A shows training loss converging rapidly - a sign of overfitting to our small 85-image dataset. Panel B reveals validation Dice peaks at epoch 8 then declines as the model forgets pretrained features. Panel C confirms: zero-shot outperforms all finetuned variants. This is catastrophic forgetting on a small medical dataset."

---

## âœ… PRESENTATION CHECKLIST

- [ ] Load `fig4_method_overview.png` for methods slide
- [ ] Load `fig1_segmentation_comprehensive.png` for main results
- [ ] Load `fig2_clip_analysis.png` for CLIP results  
- [ ] Load `fig3_training_analysis.png` for training analysis
- [ ] Load `fig5_summary_results.png` for summary
- [ ] Memorize: 0.555, -33%, +64%, 44.4%
- [ ] Practice 6-minute timing (aim for 5:30)

---

## ğŸ“ FILE LOCATIONS

```
results/figures/academic/
â”œâ”€â”€ fig1_segmentation_comprehensive.png  â† MAIN RESULTS
â”œâ”€â”€ fig2_clip_analysis.png               â† CLIP RESULTS
â”œâ”€â”€ fig3_training_analysis.png           â† TRAINING ANALYSIS
â”œâ”€â”€ fig4_method_overview.png             â† PIPELINE
â””â”€â”€ fig5_summary_results.png             â† SUMMARY TABLES

All figures also available as PDF for high-quality printing.
```
